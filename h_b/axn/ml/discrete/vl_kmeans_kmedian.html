<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.4" />
<title>axn.ml.discrete.vl_kmeans_kmedian API documentation</title>
<meta name="description" content="K_Means Clustering is an unsupervised machine learning method that segments similar data points into groups â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>axn.ml.discrete.vl_kmeans_kmedian</code></h1>
</header>
<section id="section-intro">
<p>K_Means Clustering is an unsupervised machine learning method that segments similar data points into groups.</p>
<p>It's considered unsupervised because there's no ground truth value to predict.
Instead, we're trying to create structure/meaning from the data.</p>
<p>K-Means Clustering Explanation</p>
<p>1) Assign k value as the number of desired clusters.</p>
<p>2) Randomly assign centroids of clusters from points in our dataset.</p>
<p>3) Assign each dataset point to the nearest centroid based on the Euclidean distance metric; this creates clusters.</p>
<ul>
<li>Euclidean distance computes the distance between two objects using the Pythagorean Theorem.
If you walked three blocks North and four blocks West,
your Euclidean distance is five blocks.</li>
</ul>
<p>4) Move centroids to the mean value of the clustered dataset points.</p>
<p>5) Iterate/repeat steps 3-4 until centroids don't move or we reach our
maximum number of iterations allowed (called convergence).</p>
<p>Optionally, you could repeat steps 2-5 a fixed number of times (such as 10).
With each new random initialization of centroids, you may get slightly different results.
With different results, you'll likely have different centroids and slightly
different dataset points in each cluster.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;

K_Means Clustering is an unsupervised machine learning method that segments similar data points into groups.

It&#39;s considered unsupervised because there&#39;s no ground truth value to predict.
Instead, we&#39;re trying to create structure/meaning from the data.

K-Means Clustering Explanation


1) Assign k value as the number of desired clusters.

2) Randomly assign centroids of clusters from points in our dataset.

3) Assign each dataset point to the nearest centroid based on the Euclidean distance metric; this creates clusters.

- Euclidean distance computes the distance between two objects using the Pythagorean Theorem.
If you walked three blocks North and four blocks West,
your Euclidean distance is five blocks.

4) Move centroids to the mean value of the clustered dataset points.

5) Iterate/repeat steps 3-4 until centroids don&#39;t move or we reach our
maximum number of iterations allowed (called convergence).

Optionally, you could repeat steps 2-5 a fixed number of times (such as 10).
With each new random initialization of centroids, you may get slightly different results.
With different results, you&#39;ll likely have different centroids and slightly
different dataset points in each cluster.

&#34;&#34;&#34;
from collections import defaultdict
import random
from sklearn.preprocessing import MinMaxScaler
import numpy as np
import utils


class Kmeans:
    &#34;&#34;&#34;
Clustering is an unsupervised machine learning method that segments similar data points into groups.

It&#39;s considered unsupervised because there&#39;s no ground truth value to predict.
Instead, we&#39;re trying to create structure/meaning from the data.

K-Means Clustering Explanation


1) Assign k value as the number of desired clusters.

2) Randomly assign centroids of clusters from points in our dataset.

3) Assign each dataset point to the nearest centroid based on the Euclidean distance metric;
this creates clusters.

- Euclidean distance computes the distance between two objects using the Pythagorean Theorem.
If you walked three blocks North and four blocks West,
your Euclidean distance is five blocks.

4) Move centroids to the mean value of the clustered dataset points.

5) Iterate/repeat steps 3-4 until centroids don&#39;t move or we reach our
maximum number of iterations allowed (called convergence).

Optionally, you could repeat steps 2-5 a fixed number of times (such as 10).
With each new random initialization of centroids, you may get slightly different results.
With different results, you&#39;ll likely have different centroids
and slightly different dataset points in each cluster.

      &#34;&#34;&#34;

    # pylint: disable=no-member

    def __init__(self, k=4, tol=0.001, max_iter=300):

        self.k = k

        self.tol = tol
        self.max_iter = max_iter
        self.centroids = {}
        self.classifications = {}

    def fit(self, data):
        &#34;&#34;&#34;
        Fit the estimator.

        Parameters
        ----------
        X : numeric array-like, shape (n_samples, n_features)
            Data to be discretized.

        y : None
            Ignored. This parameter exists only for compatibility with
            :class:`sklearn.pipeline.Pipeline`.

        Returns
        -------
        self
        &#34;&#34;&#34;
        # pylint: disable=too-many-locals
        # pylint: disable=redefined-outer-name
        # pylint: disable=too-many-instance-attributes


        # edge_array must defauilt to []
        self.centroids = {}

        for i in range(self.k):
            self.centroids[i] = data[i]

        for i in range(self.max_iter):
            self.classifications = {}

            for i in range(self.k):
                self.classifications[i] = []

            for featureset in data:
                distances = [
                    np.linalg.norm(
                        featureset -
                        self.centroids[centroid]) for centroid in self.centroids]
                classification = distances.index(min(distances))
                self.classifications[classification].append(featureset)

            prev_centroids = dict(self.centroids)

            for classification in self.classifications:
                # replace with median
                self.centroids[classification] = np.average(
                    self.classifications[classification], axis=0)

            optimized = True

            for c_my in self.centroids:
                original_centroid = prev_centroids[c_my]
                current_centroid = self.centroids[c_my]
                if np.sum((current_centroid - original_centroid) /
                          original_centroid * 100.0) &gt; self.tol:
                    print(&#34;ITER &#34; + str(np.sum((current_centroid -
                                                original_centroid) / original_centroid * 100.0)))
                    optimized = False

            if optimized:
                break

    def predict(self, data):
        &#34;&#34;&#34;
        prediction entry point where linear algebra is used to measure group distance
        located groups - of dataset points.
        &#34;&#34;&#34;
        distances = [np.linalg.norm(data - self.centroids[centroid])
                     for centroid in self.centroids]
        classification = distances.index(min(distances))
        return classification


class KmeansAssign():
    &#34;&#34;&#34;
    Calculations associated with K-Means clustering on a set of n-dimensional data points to find clusters - closely
    located groups - of dataset points.
    &#34;&#34;&#34;
    # pylint: disable=useless-return
    # pylint: disable=too-many-instance-attributes
    # pylint: disable=unused-variable


    def __init__(
            self,
            dataset_numpy_array,
            k_number_of_clusters,
            number_of_centroid_initializations,
            max_number_of_iterations=30):
        &#34;&#34;&#34;
        Attributes associated with all K-Means clustering of data points
        :param dataset_numpy_array: numpy array of n-dimensional points you&#39;d like to cluster
        :param k_number_of_clusters: number of clusters to create
        :param max_number_of_iterations: maximum number of possible iterations to run K-Means
        &#34;&#34;&#34;
        self.dataset = dataset_numpy_array
        self.k_number_of_clusters = k_number_of_clusters
        self.number_of_instances, self.number_of_features = self.dataset.shape
        self.number_of_centroid_initializations = number_of_centroid_initializations
        self.inertia_values = []
        self.max_number_of_iterations = max_number_of_iterations
        # all centroids and clustered dataset points
        self.clusters_all_iterations_record = []

    @staticmethod
    def get_euclidean_distance(
            n_dimensional_numpy_array_0,
            n_dimensional_numpy_array_1):
        &#34;&#34;&#34;
        Static method to calculate the normalized Euclidean distance between any n-dimensional numpy arrays
        :param n_dimensional_numpy_array_0: one n-dimensional numpy array (aka a point in space)
        :param n_dimensional_numpy_array_1: another n-dimensional numpy array (aka a point in space)
        :return: magnitude of Euclidean distance between two n-dimensional numpy arrays; scalar value
        &#34;&#34;&#34;
        return np.linalg.norm(
            n_dimensional_numpy_array_0 -
            n_dimensional_numpy_array_1)

    def create_random_initial_centroids(self):
        &#34;&#34;&#34;
        Create random initial centroids based on dataset; creates # of centroids to match # of clusters
        :return:
        &#34;&#34;&#34;
        random_dataset_indices = random.sample(
            range(0, self.number_of_instances), self.k_number_of_clusters)
        random_initial_centroids = self.dataset[random_dataset_indices]
        return random_initial_centroids

    def assign_dataset_points_to_closest_centroid(self, centroids):
        &#34;&#34;&#34;
        Given any number of centroid values, assign each point to its closest centroid based on the Euclidean distance
        metric. Use data structure cluster_iteration_record to keep track of the centroid and associated points in a
        single iteration.
        :param centroids: numpy array of centroid values
        :return: record of centroid and associated dataset points in its cluster for a single K-Means iteration
        &#34;&#34;&#34;
        cluster_single_iteration_record = defaultdict(list)
        for dataset_point in self.dataset:
            euclidean_distances_between_dataset_point_and_centroids = []
            for centroid in centroids:
                distance_between_centroid_and_dataset_point = self.get_euclidean_distance(
                    centroid, dataset_point)

                euclidean_distances_between_dataset_point_and_centroids.append(
                    distance_between_centroid_and_dataset_point)
            index_of_closest_centroid = np.argmin(
                euclidean_distances_between_dataset_point_and_centroids)
            closest_centroid = tuple(centroids[index_of_closest_centroid])
            cluster_single_iteration_record[closest_centroid].append(
                dataset_point)
        return cluster_single_iteration_record

    def run_kmeans_initialized_centroid(self, initialization_number):
        &#34;&#34;&#34;
        Assign dataset points to clusters based on nearest centroid; update centroids based on mean of cluster points.
        Repeat steps above until centroids don&#39;t move or we&#39;ve reached max_number_of_iterations.

        :return: None
        &#34;&#34;&#34;
        centroids = self.create_random_initial_centroids()
        # list of record of iteration centroids and clustered points
        self.clusters_all_iterations_record.append([])

        for iteration in range(1, self.max_number_of_iterations + 1):
            cluster_single_iteration_record = self.assign_dataset_points_to_closest_centroid(
                centroids=centroids)
            self.clusters_all_iterations_record[initialization_number].append(
                cluster_single_iteration_record)
            updated_centroids = []
            for centroid in cluster_single_iteration_record:
                cluster_dataset_points = cluster_single_iteration_record[centroid]

                updated_centroid = np.mean(cluster_dataset_points, axis=0)
                updated_centroids.append(updated_centroid)
            if self.get_euclidean_distance(
                    np.array(updated_centroids), centroids) == 0:

                break
            centroids = updated_centroids
        return None

    def fit(self):
        &#34;&#34;&#34;
        Implements K-Means the max number_of_centroid_initializations times; each time, there&#39;s new initial centroids.
        :return: None
        &#34;&#34;&#34;

        for initialization_number in range(
                self.number_of_centroid_initializations):
            self.run_kmeans_initialized_centroid(
                initialization_number=initialization_number)

            # index of -1 is for the last cluster assignment of the iteration
            inertia_of_last_cluster_record = self.inertia(
                self.clusters_all_iterations_record[initialization_number][-1])
            self.inertia_values.append(inertia_of_last_cluster_record)
        return None

    def inertia(self, clusters):
        &#34;&#34;&#34;
        Get the sum of squared distances of dataset points to their cluster centers for all clusters - defined as inertia
        :return: cluster_sum_of_squares_points_to_clusters
        &#34;&#34;&#34;
        cluster_sum_of_squares_points_to_clusters = 0

        for centroid, cluster_points in clusters.items():

            for cluster_point in cluster_points:
                euclidean_norm_distance = self.get_euclidean_distance(
                    cluster_point, centroid)
                euclidean_norm_distance_squared = euclidean_norm_distance ** 2

                cluster_sum_of_squares_points_to_clusters += euclidean_norm_distance_squared
        return cluster_sum_of_squares_points_to_clusters

    def index_lowest_inertia_cluster(self):
        &#34;&#34;&#34;
        In our list of inertia_values, finds the index of the minimum inertia
        :return: index_lowest_inertia
        &#34;&#34;&#34;
        minimum_inertia_value = min(self.inertia_values)
        index_lowest_inertia = self.inertia_values.index(minimum_inertia_value)
        return index_lowest_inertia

    def final_iteration_optimal_cluster(self):
        &#34;&#34;&#34;
        Get results of optimal cluster assignment based  on the lowest inertia value
        :return: dictionary with keys as centroids and values as list of dataset points in the clusters
        &#34;&#34;&#34;
        # -1 gets us the final iteration from a centroid initialization of running K-Means
        return self.clusters_all_iterations_record[self.index_lowest_inertia_cluster(
        )][-1]

    def final_iteration_optimal_cluster_centroids(self):
        &#34;&#34;&#34;
        Get centroids of the optimal cluster assignment based on the lowest inertia value
        :return: list of tuples with tuples holding centroid locations
        &#34;&#34;&#34;
        return list(self.final_iteration_optimal_cluster().keys())

    def predict(self, n_dimensional_numpy_array):
        &#34;&#34;&#34;
        Predict which cluster a new point belongs to; calculates euclidean distance from point to all centroids
        :param n_dimensional_numpy_array: new observation that has same n-dimensions as dataset points
        :return: closest_centroid
        &#34;&#34;&#34;
        # initially assign closest_centroid as large value; we&#39;ll reassign it
        # later
        closest_centroid = np.inf
        for centroid in self.final_iteration_optimal_cluster_centroids():
            distance = self.get_euclidean_distance(
                centroid, n_dimensional_numpy_array)
            if distance &lt; closest_centroid:
                closest_centroid = centroid
        return closest_centroid


normalizer = MinMaxScaler()


class Kmedians:
    &#34;&#34;&#34;
    Calculations associated with K-Means clustering on a set of n-dimensional data points to find clusters - closely
    located groups - of dataset points.
    &#34;&#34;&#34;
    # pylint: disable=useless-return
    # pylint: disable=too-many-instance-attributes
    # pylint: disable=invalid-name
    # pylint: disable=no-member

    def __init__(self, k):
        self.k = k
        self.medians = []

    def fit(self, X):
        &#34;&#34;&#34;
        Fit the estimator.

        Parameters
        ----------
        X : numeric array-like, shape (n_samples, n_features)
            Data to be discretized.

        y : None
            Ignored. This parameter exists only for compatibility with
            :class:`sklearn.pipeline.Pipeline`.

        Returns
        -------
        self
        &#34;&#34;&#34;
        N, D = X.shape
        y = np.ones(N)

        medians = np.zeros((self.k, D))
        for kk in range(self.k):
            i = np.random.randint(N)
            medians[kk] = X[i]

        while True:
            y_old = y

            # Compute euclidean distance to each mean
            dist2 = utils.euclidean_dist_squared(X, medians)
            dist2[np.isnan(dist2)] = np.inf
            y = np.argmin(dist2, axis=1)

            # Update means
            for kk in range(self.k):
                medians[kk] = np.median(X[y == kk], axis=0)

            changes = np.sum(y != y_old)
            # print(&#39;Running K-means, changes in cluster assignment = {}&#39;.format(changes))

            # Stop if no point changed cluster
            if changes == 0:
                break

        self.medians = medians

    def predict(self, X):
        &#34;&#34;&#34;
        prediction entry point where linear algebra is used to measure group distance
        located groups - of dataset points.
        &#34;&#34;&#34;
        medians = self.medians
        dist2 = utils.euclidean_dist_squared(X, medians)
        dist2[np.isnan(dist2)] = np.inf
        return np.argmin(dist2, axis=1)

    def error(self, X):
        &#34;&#34;&#34;
        error entry point where linear algebra is used to measure group distance
        located groups - of dataset points.
        &#34;&#34;&#34;
        medians = self.medians
        closest_median_indexes = self.predict(X)

        error = 0
        for i in range(medians.shape[0]):
            error += np.sum(utils.euclidean_dist_squared(
                X[closest_median_indexes == i], medians[[i]]))
        return error</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="axn.ml.discrete.vl_kmeans_kmedian.Kmeans"><code class="flex name class">
<span>class <span class="ident">Kmeans</span></span>
<span>(</span><span>k=4, tol=0.001, max_iter=300)</span>
</code></dt>
<dd>
<div class="desc"><p>Clustering is an unsupervised machine learning method that segments similar data points into groups.</p>
<p>It's considered unsupervised because there's no ground truth value to predict.
Instead, we're trying to create structure/meaning from the data.</p>
<p>K-Means Clustering Explanation</p>
<p>1) Assign k value as the number of desired clusters.</p>
<p>2) Randomly assign centroids of clusters from points in our dataset.</p>
<p>3) Assign each dataset point to the nearest centroid based on the Euclidean distance metric;
this creates clusters.</p>
<ul>
<li>Euclidean distance computes the distance between two objects using the Pythagorean Theorem.
If you walked three blocks North and four blocks West,
your Euclidean distance is five blocks.</li>
</ul>
<p>4) Move centroids to the mean value of the clustered dataset points.</p>
<p>5) Iterate/repeat steps 3-4 until centroids don't move or we reach our
maximum number of iterations allowed (called convergence).</p>
<p>Optionally, you could repeat steps 2-5 a fixed number of times (such as 10).
With each new random initialization of centroids, you may get slightly different results.
With different results, you'll likely have different centroids
and slightly different dataset points in each cluster.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Kmeans:
    &#34;&#34;&#34;
Clustering is an unsupervised machine learning method that segments similar data points into groups.

It&#39;s considered unsupervised because there&#39;s no ground truth value to predict.
Instead, we&#39;re trying to create structure/meaning from the data.

K-Means Clustering Explanation


1) Assign k value as the number of desired clusters.

2) Randomly assign centroids of clusters from points in our dataset.

3) Assign each dataset point to the nearest centroid based on the Euclidean distance metric;
this creates clusters.

- Euclidean distance computes the distance between two objects using the Pythagorean Theorem.
If you walked three blocks North and four blocks West,
your Euclidean distance is five blocks.

4) Move centroids to the mean value of the clustered dataset points.

5) Iterate/repeat steps 3-4 until centroids don&#39;t move or we reach our
maximum number of iterations allowed (called convergence).

Optionally, you could repeat steps 2-5 a fixed number of times (such as 10).
With each new random initialization of centroids, you may get slightly different results.
With different results, you&#39;ll likely have different centroids
and slightly different dataset points in each cluster.

      &#34;&#34;&#34;

    # pylint: disable=no-member

    def __init__(self, k=4, tol=0.001, max_iter=300):

        self.k = k

        self.tol = tol
        self.max_iter = max_iter
        self.centroids = {}
        self.classifications = {}

    def fit(self, data):
        &#34;&#34;&#34;
        Fit the estimator.

        Parameters
        ----------
        X : numeric array-like, shape (n_samples, n_features)
            Data to be discretized.

        y : None
            Ignored. This parameter exists only for compatibility with
            :class:`sklearn.pipeline.Pipeline`.

        Returns
        -------
        self
        &#34;&#34;&#34;
        # pylint: disable=too-many-locals
        # pylint: disable=redefined-outer-name
        # pylint: disable=too-many-instance-attributes


        # edge_array must defauilt to []
        self.centroids = {}

        for i in range(self.k):
            self.centroids[i] = data[i]

        for i in range(self.max_iter):
            self.classifications = {}

            for i in range(self.k):
                self.classifications[i] = []

            for featureset in data:
                distances = [
                    np.linalg.norm(
                        featureset -
                        self.centroids[centroid]) for centroid in self.centroids]
                classification = distances.index(min(distances))
                self.classifications[classification].append(featureset)

            prev_centroids = dict(self.centroids)

            for classification in self.classifications:
                # replace with median
                self.centroids[classification] = np.average(
                    self.classifications[classification], axis=0)

            optimized = True

            for c_my in self.centroids:
                original_centroid = prev_centroids[c_my]
                current_centroid = self.centroids[c_my]
                if np.sum((current_centroid - original_centroid) /
                          original_centroid * 100.0) &gt; self.tol:
                    print(&#34;ITER &#34; + str(np.sum((current_centroid -
                                                original_centroid) / original_centroid * 100.0)))
                    optimized = False

            if optimized:
                break

    def predict(self, data):
        &#34;&#34;&#34;
        prediction entry point where linear algebra is used to measure group distance
        located groups - of dataset points.
        &#34;&#34;&#34;
        distances = [np.linalg.norm(data - self.centroids[centroid])
                     for centroid in self.centroids]
        classification = distances.index(min(distances))
        return classification</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="axn.ml.discrete.vl_kmeans_kmedian.Kmeans.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, data)</span>
</code></dt>
<dd>
<div class="desc"><p>Fit the estimator.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>numeric array-like, shape (n_samples, n_features)</code></dt>
<dd>Data to be discretized.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>None</code></dt>
<dd>Ignored. This parameter exists only for compatibility with
:class:<code>sklearn.pipeline.Pipeline</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>self</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, data):
    &#34;&#34;&#34;
    Fit the estimator.

    Parameters
    ----------
    X : numeric array-like, shape (n_samples, n_features)
        Data to be discretized.

    y : None
        Ignored. This parameter exists only for compatibility with
        :class:`sklearn.pipeline.Pipeline`.

    Returns
    -------
    self
    &#34;&#34;&#34;
    # pylint: disable=too-many-locals
    # pylint: disable=redefined-outer-name
    # pylint: disable=too-many-instance-attributes


    # edge_array must defauilt to []
    self.centroids = {}

    for i in range(self.k):
        self.centroids[i] = data[i]

    for i in range(self.max_iter):
        self.classifications = {}

        for i in range(self.k):
            self.classifications[i] = []

        for featureset in data:
            distances = [
                np.linalg.norm(
                    featureset -
                    self.centroids[centroid]) for centroid in self.centroids]
            classification = distances.index(min(distances))
            self.classifications[classification].append(featureset)

        prev_centroids = dict(self.centroids)

        for classification in self.classifications:
            # replace with median
            self.centroids[classification] = np.average(
                self.classifications[classification], axis=0)

        optimized = True

        for c_my in self.centroids:
            original_centroid = prev_centroids[c_my]
            current_centroid = self.centroids[c_my]
            if np.sum((current_centroid - original_centroid) /
                      original_centroid * 100.0) &gt; self.tol:
                print(&#34;ITER &#34; + str(np.sum((current_centroid -
                                            original_centroid) / original_centroid * 100.0)))
                optimized = False

        if optimized:
            break</code></pre>
</details>
</dd>
<dt id="axn.ml.discrete.vl_kmeans_kmedian.Kmeans.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, data)</span>
</code></dt>
<dd>
<div class="desc"><p>prediction entry point where linear algebra is used to measure group distance
located groups - of dataset points.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, data):
    &#34;&#34;&#34;
    prediction entry point where linear algebra is used to measure group distance
    located groups - of dataset points.
    &#34;&#34;&#34;
    distances = [np.linalg.norm(data - self.centroids[centroid])
                 for centroid in self.centroids]
    classification = distances.index(min(distances))
    return classification</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="axn.ml.discrete.vl_kmeans_kmedian.KmeansAssign"><code class="flex name class">
<span>class <span class="ident">KmeansAssign</span></span>
<span>(</span><span>dataset_numpy_array, k_number_of_clusters, number_of_centroid_initializations, max_number_of_iterations=30)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculations associated with K-Means clustering on a set of n-dimensional data points to find clusters - closely
located groups - of dataset points.</p>
<p>Attributes associated with all K-Means clustering of data points
:param dataset_numpy_array: numpy array of n-dimensional points you'd like to cluster
:param k_number_of_clusters: number of clusters to create
:param max_number_of_iterations: maximum number of possible iterations to run K-Means</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class KmeansAssign():
    &#34;&#34;&#34;
    Calculations associated with K-Means clustering on a set of n-dimensional data points to find clusters - closely
    located groups - of dataset points.
    &#34;&#34;&#34;
    # pylint: disable=useless-return
    # pylint: disable=too-many-instance-attributes
    # pylint: disable=unused-variable


    def __init__(
            self,
            dataset_numpy_array,
            k_number_of_clusters,
            number_of_centroid_initializations,
            max_number_of_iterations=30):
        &#34;&#34;&#34;
        Attributes associated with all K-Means clustering of data points
        :param dataset_numpy_array: numpy array of n-dimensional points you&#39;d like to cluster
        :param k_number_of_clusters: number of clusters to create
        :param max_number_of_iterations: maximum number of possible iterations to run K-Means
        &#34;&#34;&#34;
        self.dataset = dataset_numpy_array
        self.k_number_of_clusters = k_number_of_clusters
        self.number_of_instances, self.number_of_features = self.dataset.shape
        self.number_of_centroid_initializations = number_of_centroid_initializations
        self.inertia_values = []
        self.max_number_of_iterations = max_number_of_iterations
        # all centroids and clustered dataset points
        self.clusters_all_iterations_record = []

    @staticmethod
    def get_euclidean_distance(
            n_dimensional_numpy_array_0,
            n_dimensional_numpy_array_1):
        &#34;&#34;&#34;
        Static method to calculate the normalized Euclidean distance between any n-dimensional numpy arrays
        :param n_dimensional_numpy_array_0: one n-dimensional numpy array (aka a point in space)
        :param n_dimensional_numpy_array_1: another n-dimensional numpy array (aka a point in space)
        :return: magnitude of Euclidean distance between two n-dimensional numpy arrays; scalar value
        &#34;&#34;&#34;
        return np.linalg.norm(
            n_dimensional_numpy_array_0 -
            n_dimensional_numpy_array_1)

    def create_random_initial_centroids(self):
        &#34;&#34;&#34;
        Create random initial centroids based on dataset; creates # of centroids to match # of clusters
        :return:
        &#34;&#34;&#34;
        random_dataset_indices = random.sample(
            range(0, self.number_of_instances), self.k_number_of_clusters)
        random_initial_centroids = self.dataset[random_dataset_indices]
        return random_initial_centroids

    def assign_dataset_points_to_closest_centroid(self, centroids):
        &#34;&#34;&#34;
        Given any number of centroid values, assign each point to its closest centroid based on the Euclidean distance
        metric. Use data structure cluster_iteration_record to keep track of the centroid and associated points in a
        single iteration.
        :param centroids: numpy array of centroid values
        :return: record of centroid and associated dataset points in its cluster for a single K-Means iteration
        &#34;&#34;&#34;
        cluster_single_iteration_record = defaultdict(list)
        for dataset_point in self.dataset:
            euclidean_distances_between_dataset_point_and_centroids = []
            for centroid in centroids:
                distance_between_centroid_and_dataset_point = self.get_euclidean_distance(
                    centroid, dataset_point)

                euclidean_distances_between_dataset_point_and_centroids.append(
                    distance_between_centroid_and_dataset_point)
            index_of_closest_centroid = np.argmin(
                euclidean_distances_between_dataset_point_and_centroids)
            closest_centroid = tuple(centroids[index_of_closest_centroid])
            cluster_single_iteration_record[closest_centroid].append(
                dataset_point)
        return cluster_single_iteration_record

    def run_kmeans_initialized_centroid(self, initialization_number):
        &#34;&#34;&#34;
        Assign dataset points to clusters based on nearest centroid; update centroids based on mean of cluster points.
        Repeat steps above until centroids don&#39;t move or we&#39;ve reached max_number_of_iterations.

        :return: None
        &#34;&#34;&#34;
        centroids = self.create_random_initial_centroids()
        # list of record of iteration centroids and clustered points
        self.clusters_all_iterations_record.append([])

        for iteration in range(1, self.max_number_of_iterations + 1):
            cluster_single_iteration_record = self.assign_dataset_points_to_closest_centroid(
                centroids=centroids)
            self.clusters_all_iterations_record[initialization_number].append(
                cluster_single_iteration_record)
            updated_centroids = []
            for centroid in cluster_single_iteration_record:
                cluster_dataset_points = cluster_single_iteration_record[centroid]

                updated_centroid = np.mean(cluster_dataset_points, axis=0)
                updated_centroids.append(updated_centroid)
            if self.get_euclidean_distance(
                    np.array(updated_centroids), centroids) == 0:

                break
            centroids = updated_centroids
        return None

    def fit(self):
        &#34;&#34;&#34;
        Implements K-Means the max number_of_centroid_initializations times; each time, there&#39;s new initial centroids.
        :return: None
        &#34;&#34;&#34;

        for initialization_number in range(
                self.number_of_centroid_initializations):
            self.run_kmeans_initialized_centroid(
                initialization_number=initialization_number)

            # index of -1 is for the last cluster assignment of the iteration
            inertia_of_last_cluster_record = self.inertia(
                self.clusters_all_iterations_record[initialization_number][-1])
            self.inertia_values.append(inertia_of_last_cluster_record)
        return None

    def inertia(self, clusters):
        &#34;&#34;&#34;
        Get the sum of squared distances of dataset points to their cluster centers for all clusters - defined as inertia
        :return: cluster_sum_of_squares_points_to_clusters
        &#34;&#34;&#34;
        cluster_sum_of_squares_points_to_clusters = 0

        for centroid, cluster_points in clusters.items():

            for cluster_point in cluster_points:
                euclidean_norm_distance = self.get_euclidean_distance(
                    cluster_point, centroid)
                euclidean_norm_distance_squared = euclidean_norm_distance ** 2

                cluster_sum_of_squares_points_to_clusters += euclidean_norm_distance_squared
        return cluster_sum_of_squares_points_to_clusters

    def index_lowest_inertia_cluster(self):
        &#34;&#34;&#34;
        In our list of inertia_values, finds the index of the minimum inertia
        :return: index_lowest_inertia
        &#34;&#34;&#34;
        minimum_inertia_value = min(self.inertia_values)
        index_lowest_inertia = self.inertia_values.index(minimum_inertia_value)
        return index_lowest_inertia

    def final_iteration_optimal_cluster(self):
        &#34;&#34;&#34;
        Get results of optimal cluster assignment based  on the lowest inertia value
        :return: dictionary with keys as centroids and values as list of dataset points in the clusters
        &#34;&#34;&#34;
        # -1 gets us the final iteration from a centroid initialization of running K-Means
        return self.clusters_all_iterations_record[self.index_lowest_inertia_cluster(
        )][-1]

    def final_iteration_optimal_cluster_centroids(self):
        &#34;&#34;&#34;
        Get centroids of the optimal cluster assignment based on the lowest inertia value
        :return: list of tuples with tuples holding centroid locations
        &#34;&#34;&#34;
        return list(self.final_iteration_optimal_cluster().keys())

    def predict(self, n_dimensional_numpy_array):
        &#34;&#34;&#34;
        Predict which cluster a new point belongs to; calculates euclidean distance from point to all centroids
        :param n_dimensional_numpy_array: new observation that has same n-dimensions as dataset points
        :return: closest_centroid
        &#34;&#34;&#34;
        # initially assign closest_centroid as large value; we&#39;ll reassign it
        # later
        closest_centroid = np.inf
        for centroid in self.final_iteration_optimal_cluster_centroids():
            distance = self.get_euclidean_distance(
                centroid, n_dimensional_numpy_array)
            if distance &lt; closest_centroid:
                closest_centroid = centroid
        return closest_centroid</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="axn.ml.discrete.vl_kmeans_kmedian.KmeansAssign.get_euclidean_distance"><code class="name flex">
<span>def <span class="ident">get_euclidean_distance</span></span>(<span>n_dimensional_numpy_array_0, n_dimensional_numpy_array_1)</span>
</code></dt>
<dd>
<div class="desc"><p>Static method to calculate the normalized Euclidean distance between any n-dimensional numpy arrays
:param n_dimensional_numpy_array_0: one n-dimensional numpy array (aka a point in space)
:param n_dimensional_numpy_array_1: another n-dimensional numpy array (aka a point in space)
:return: magnitude of Euclidean distance between two n-dimensional numpy arrays; scalar value</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def get_euclidean_distance(
        n_dimensional_numpy_array_0,
        n_dimensional_numpy_array_1):
    &#34;&#34;&#34;
    Static method to calculate the normalized Euclidean distance between any n-dimensional numpy arrays
    :param n_dimensional_numpy_array_0: one n-dimensional numpy array (aka a point in space)
    :param n_dimensional_numpy_array_1: another n-dimensional numpy array (aka a point in space)
    :return: magnitude of Euclidean distance between two n-dimensional numpy arrays; scalar value
    &#34;&#34;&#34;
    return np.linalg.norm(
        n_dimensional_numpy_array_0 -
        n_dimensional_numpy_array_1)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="axn.ml.discrete.vl_kmeans_kmedian.KmeansAssign.assign_dataset_points_to_closest_centroid"><code class="name flex">
<span>def <span class="ident">assign_dataset_points_to_closest_centroid</span></span>(<span>self, centroids)</span>
</code></dt>
<dd>
<div class="desc"><p>Given any number of centroid values, assign each point to its closest centroid based on the Euclidean distance
metric. Use data structure cluster_iteration_record to keep track of the centroid and associated points in a
single iteration.
:param centroids: numpy array of centroid values
:return: record of centroid and associated dataset points in its cluster for a single K-Means iteration</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def assign_dataset_points_to_closest_centroid(self, centroids):
    &#34;&#34;&#34;
    Given any number of centroid values, assign each point to its closest centroid based on the Euclidean distance
    metric. Use data structure cluster_iteration_record to keep track of the centroid and associated points in a
    single iteration.
    :param centroids: numpy array of centroid values
    :return: record of centroid and associated dataset points in its cluster for a single K-Means iteration
    &#34;&#34;&#34;
    cluster_single_iteration_record = defaultdict(list)
    for dataset_point in self.dataset:
        euclidean_distances_between_dataset_point_and_centroids = []
        for centroid in centroids:
            distance_between_centroid_and_dataset_point = self.get_euclidean_distance(
                centroid, dataset_point)

            euclidean_distances_between_dataset_point_and_centroids.append(
                distance_between_centroid_and_dataset_point)
        index_of_closest_centroid = np.argmin(
            euclidean_distances_between_dataset_point_and_centroids)
        closest_centroid = tuple(centroids[index_of_closest_centroid])
        cluster_single_iteration_record[closest_centroid].append(
            dataset_point)
    return cluster_single_iteration_record</code></pre>
</details>
</dd>
<dt id="axn.ml.discrete.vl_kmeans_kmedian.KmeansAssign.create_random_initial_centroids"><code class="name flex">
<span>def <span class="ident">create_random_initial_centroids</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Create random initial centroids based on dataset; creates # of centroids to match # of clusters
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_random_initial_centroids(self):
    &#34;&#34;&#34;
    Create random initial centroids based on dataset; creates # of centroids to match # of clusters
    :return:
    &#34;&#34;&#34;
    random_dataset_indices = random.sample(
        range(0, self.number_of_instances), self.k_number_of_clusters)
    random_initial_centroids = self.dataset[random_dataset_indices]
    return random_initial_centroids</code></pre>
</details>
</dd>
<dt id="axn.ml.discrete.vl_kmeans_kmedian.KmeansAssign.final_iteration_optimal_cluster"><code class="name flex">
<span>def <span class="ident">final_iteration_optimal_cluster</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Get results of optimal cluster assignment based
on the lowest inertia value
:return: dictionary with keys as centroids and values as list of dataset points in the clusters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def final_iteration_optimal_cluster(self):
    &#34;&#34;&#34;
    Get results of optimal cluster assignment based  on the lowest inertia value
    :return: dictionary with keys as centroids and values as list of dataset points in the clusters
    &#34;&#34;&#34;
    # -1 gets us the final iteration from a centroid initialization of running K-Means
    return self.clusters_all_iterations_record[self.index_lowest_inertia_cluster(
    )][-1]</code></pre>
</details>
</dd>
<dt id="axn.ml.discrete.vl_kmeans_kmedian.KmeansAssign.final_iteration_optimal_cluster_centroids"><code class="name flex">
<span>def <span class="ident">final_iteration_optimal_cluster_centroids</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Get centroids of the optimal cluster assignment based on the lowest inertia value
:return: list of tuples with tuples holding centroid locations</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def final_iteration_optimal_cluster_centroids(self):
    &#34;&#34;&#34;
    Get centroids of the optimal cluster assignment based on the lowest inertia value
    :return: list of tuples with tuples holding centroid locations
    &#34;&#34;&#34;
    return list(self.final_iteration_optimal_cluster().keys())</code></pre>
</details>
</dd>
<dt id="axn.ml.discrete.vl_kmeans_kmedian.KmeansAssign.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Implements K-Means the max number_of_centroid_initializations times; each time, there's new initial centroids.
:return: None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self):
    &#34;&#34;&#34;
    Implements K-Means the max number_of_centroid_initializations times; each time, there&#39;s new initial centroids.
    :return: None
    &#34;&#34;&#34;

    for initialization_number in range(
            self.number_of_centroid_initializations):
        self.run_kmeans_initialized_centroid(
            initialization_number=initialization_number)

        # index of -1 is for the last cluster assignment of the iteration
        inertia_of_last_cluster_record = self.inertia(
            self.clusters_all_iterations_record[initialization_number][-1])
        self.inertia_values.append(inertia_of_last_cluster_record)
    return None</code></pre>
</details>
</dd>
<dt id="axn.ml.discrete.vl_kmeans_kmedian.KmeansAssign.index_lowest_inertia_cluster"><code class="name flex">
<span>def <span class="ident">index_lowest_inertia_cluster</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>In our list of inertia_values, finds the index of the minimum inertia
:return: index_lowest_inertia</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def index_lowest_inertia_cluster(self):
    &#34;&#34;&#34;
    In our list of inertia_values, finds the index of the minimum inertia
    :return: index_lowest_inertia
    &#34;&#34;&#34;
    minimum_inertia_value = min(self.inertia_values)
    index_lowest_inertia = self.inertia_values.index(minimum_inertia_value)
    return index_lowest_inertia</code></pre>
</details>
</dd>
<dt id="axn.ml.discrete.vl_kmeans_kmedian.KmeansAssign.inertia"><code class="name flex">
<span>def <span class="ident">inertia</span></span>(<span>self, clusters)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the sum of squared distances of dataset points to their cluster centers for all clusters - defined as inertia
:return: cluster_sum_of_squares_points_to_clusters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def inertia(self, clusters):
    &#34;&#34;&#34;
    Get the sum of squared distances of dataset points to their cluster centers for all clusters - defined as inertia
    :return: cluster_sum_of_squares_points_to_clusters
    &#34;&#34;&#34;
    cluster_sum_of_squares_points_to_clusters = 0

    for centroid, cluster_points in clusters.items():

        for cluster_point in cluster_points:
            euclidean_norm_distance = self.get_euclidean_distance(
                cluster_point, centroid)
            euclidean_norm_distance_squared = euclidean_norm_distance ** 2

            cluster_sum_of_squares_points_to_clusters += euclidean_norm_distance_squared
    return cluster_sum_of_squares_points_to_clusters</code></pre>
</details>
</dd>
<dt id="axn.ml.discrete.vl_kmeans_kmedian.KmeansAssign.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, n_dimensional_numpy_array)</span>
</code></dt>
<dd>
<div class="desc"><p>Predict which cluster a new point belongs to; calculates euclidean distance from point to all centroids
:param n_dimensional_numpy_array: new observation that has same n-dimensions as dataset points
:return: closest_centroid</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, n_dimensional_numpy_array):
    &#34;&#34;&#34;
    Predict which cluster a new point belongs to; calculates euclidean distance from point to all centroids
    :param n_dimensional_numpy_array: new observation that has same n-dimensions as dataset points
    :return: closest_centroid
    &#34;&#34;&#34;
    # initially assign closest_centroid as large value; we&#39;ll reassign it
    # later
    closest_centroid = np.inf
    for centroid in self.final_iteration_optimal_cluster_centroids():
        distance = self.get_euclidean_distance(
            centroid, n_dimensional_numpy_array)
        if distance &lt; closest_centroid:
            closest_centroid = centroid
    return closest_centroid</code></pre>
</details>
</dd>
<dt id="axn.ml.discrete.vl_kmeans_kmedian.KmeansAssign.run_kmeans_initialized_centroid"><code class="name flex">
<span>def <span class="ident">run_kmeans_initialized_centroid</span></span>(<span>self, initialization_number)</span>
</code></dt>
<dd>
<div class="desc"><p>Assign dataset points to clusters based on nearest centroid; update centroids based on mean of cluster points.
Repeat steps above until centroids don't move or we've reached max_number_of_iterations.</p>
<p>:return: None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_kmeans_initialized_centroid(self, initialization_number):
    &#34;&#34;&#34;
    Assign dataset points to clusters based on nearest centroid; update centroids based on mean of cluster points.
    Repeat steps above until centroids don&#39;t move or we&#39;ve reached max_number_of_iterations.

    :return: None
    &#34;&#34;&#34;
    centroids = self.create_random_initial_centroids()
    # list of record of iteration centroids and clustered points
    self.clusters_all_iterations_record.append([])

    for iteration in range(1, self.max_number_of_iterations + 1):
        cluster_single_iteration_record = self.assign_dataset_points_to_closest_centroid(
            centroids=centroids)
        self.clusters_all_iterations_record[initialization_number].append(
            cluster_single_iteration_record)
        updated_centroids = []
        for centroid in cluster_single_iteration_record:
            cluster_dataset_points = cluster_single_iteration_record[centroid]

            updated_centroid = np.mean(cluster_dataset_points, axis=0)
            updated_centroids.append(updated_centroid)
        if self.get_euclidean_distance(
                np.array(updated_centroids), centroids) == 0:

            break
        centroids = updated_centroids
    return None</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="axn.ml.discrete.vl_kmeans_kmedian.Kmedians"><code class="flex name class">
<span>class <span class="ident">Kmedians</span></span>
<span>(</span><span>k)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculations associated with K-Means clustering on a set of n-dimensional data points to find clusters - closely
located groups - of dataset points.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Kmedians:
    &#34;&#34;&#34;
    Calculations associated with K-Means clustering on a set of n-dimensional data points to find clusters - closely
    located groups - of dataset points.
    &#34;&#34;&#34;
    # pylint: disable=useless-return
    # pylint: disable=too-many-instance-attributes
    # pylint: disable=invalid-name
    # pylint: disable=no-member

    def __init__(self, k):
        self.k = k
        self.medians = []

    def fit(self, X):
        &#34;&#34;&#34;
        Fit the estimator.

        Parameters
        ----------
        X : numeric array-like, shape (n_samples, n_features)
            Data to be discretized.

        y : None
            Ignored. This parameter exists only for compatibility with
            :class:`sklearn.pipeline.Pipeline`.

        Returns
        -------
        self
        &#34;&#34;&#34;
        N, D = X.shape
        y = np.ones(N)

        medians = np.zeros((self.k, D))
        for kk in range(self.k):
            i = np.random.randint(N)
            medians[kk] = X[i]

        while True:
            y_old = y

            # Compute euclidean distance to each mean
            dist2 = utils.euclidean_dist_squared(X, medians)
            dist2[np.isnan(dist2)] = np.inf
            y = np.argmin(dist2, axis=1)

            # Update means
            for kk in range(self.k):
                medians[kk] = np.median(X[y == kk], axis=0)

            changes = np.sum(y != y_old)
            # print(&#39;Running K-means, changes in cluster assignment = {}&#39;.format(changes))

            # Stop if no point changed cluster
            if changes == 0:
                break

        self.medians = medians

    def predict(self, X):
        &#34;&#34;&#34;
        prediction entry point where linear algebra is used to measure group distance
        located groups - of dataset points.
        &#34;&#34;&#34;
        medians = self.medians
        dist2 = utils.euclidean_dist_squared(X, medians)
        dist2[np.isnan(dist2)] = np.inf
        return np.argmin(dist2, axis=1)

    def error(self, X):
        &#34;&#34;&#34;
        error entry point where linear algebra is used to measure group distance
        located groups - of dataset points.
        &#34;&#34;&#34;
        medians = self.medians
        closest_median_indexes = self.predict(X)

        error = 0
        for i in range(medians.shape[0]):
            error += np.sum(utils.euclidean_dist_squared(
                X[closest_median_indexes == i], medians[[i]]))
        return error</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="axn.ml.discrete.vl_kmeans_kmedian.Kmedians.error"><code class="name flex">
<span>def <span class="ident">error</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"><p>error entry point where linear algebra is used to measure group distance
located groups - of dataset points.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def error(self, X):
    &#34;&#34;&#34;
    error entry point where linear algebra is used to measure group distance
    located groups - of dataset points.
    &#34;&#34;&#34;
    medians = self.medians
    closest_median_indexes = self.predict(X)

    error = 0
    for i in range(medians.shape[0]):
        error += np.sum(utils.euclidean_dist_squared(
            X[closest_median_indexes == i], medians[[i]]))
    return error</code></pre>
</details>
</dd>
<dt id="axn.ml.discrete.vl_kmeans_kmedian.Kmedians.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"><p>Fit the estimator.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>numeric array-like, shape (n_samples, n_features)</code></dt>
<dd>Data to be discretized.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>None</code></dt>
<dd>Ignored. This parameter exists only for compatibility with
:class:<code>sklearn.pipeline.Pipeline</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>self</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, X):
    &#34;&#34;&#34;
    Fit the estimator.

    Parameters
    ----------
    X : numeric array-like, shape (n_samples, n_features)
        Data to be discretized.

    y : None
        Ignored. This parameter exists only for compatibility with
        :class:`sklearn.pipeline.Pipeline`.

    Returns
    -------
    self
    &#34;&#34;&#34;
    N, D = X.shape
    y = np.ones(N)

    medians = np.zeros((self.k, D))
    for kk in range(self.k):
        i = np.random.randint(N)
        medians[kk] = X[i]

    while True:
        y_old = y

        # Compute euclidean distance to each mean
        dist2 = utils.euclidean_dist_squared(X, medians)
        dist2[np.isnan(dist2)] = np.inf
        y = np.argmin(dist2, axis=1)

        # Update means
        for kk in range(self.k):
            medians[kk] = np.median(X[y == kk], axis=0)

        changes = np.sum(y != y_old)
        # print(&#39;Running K-means, changes in cluster assignment = {}&#39;.format(changes))

        # Stop if no point changed cluster
        if changes == 0:
            break

    self.medians = medians</code></pre>
</details>
</dd>
<dt id="axn.ml.discrete.vl_kmeans_kmedian.Kmedians.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"><p>prediction entry point where linear algebra is used to measure group distance
located groups - of dataset points.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, X):
    &#34;&#34;&#34;
    prediction entry point where linear algebra is used to measure group distance
    located groups - of dataset points.
    &#34;&#34;&#34;
    medians = self.medians
    dist2 = utils.euclidean_dist_squared(X, medians)
    dist2[np.isnan(dist2)] = np.inf
    return np.argmin(dist2, axis=1)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="axn.ml.discrete" href="index.html">axn.ml.discrete</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="axn.ml.discrete.vl_kmeans_kmedian.Kmeans" href="#axn.ml.discrete.vl_kmeans_kmedian.Kmeans">Kmeans</a></code></h4>
<ul class="">
<li><code><a title="axn.ml.discrete.vl_kmeans_kmedian.Kmeans.fit" href="#axn.ml.discrete.vl_kmeans_kmedian.Kmeans.fit">fit</a></code></li>
<li><code><a title="axn.ml.discrete.vl_kmeans_kmedian.Kmeans.predict" href="#axn.ml.discrete.vl_kmeans_kmedian.Kmeans.predict">predict</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="axn.ml.discrete.vl_kmeans_kmedian.KmeansAssign" href="#axn.ml.discrete.vl_kmeans_kmedian.KmeansAssign">KmeansAssign</a></code></h4>
<ul class="">
<li><code><a title="axn.ml.discrete.vl_kmeans_kmedian.KmeansAssign.assign_dataset_points_to_closest_centroid" href="#axn.ml.discrete.vl_kmeans_kmedian.KmeansAssign.assign_dataset_points_to_closest_centroid">assign_dataset_points_to_closest_centroid</a></code></li>
<li><code><a title="axn.ml.discrete.vl_kmeans_kmedian.KmeansAssign.create_random_initial_centroids" href="#axn.ml.discrete.vl_kmeans_kmedian.KmeansAssign.create_random_initial_centroids">create_random_initial_centroids</a></code></li>
<li><code><a title="axn.ml.discrete.vl_kmeans_kmedian.KmeansAssign.final_iteration_optimal_cluster" href="#axn.ml.discrete.vl_kmeans_kmedian.KmeansAssign.final_iteration_optimal_cluster">final_iteration_optimal_cluster</a></code></li>
<li><code><a title="axn.ml.discrete.vl_kmeans_kmedian.KmeansAssign.final_iteration_optimal_cluster_centroids" href="#axn.ml.discrete.vl_kmeans_kmedian.KmeansAssign.final_iteration_optimal_cluster_centroids">final_iteration_optimal_cluster_centroids</a></code></li>
<li><code><a title="axn.ml.discrete.vl_kmeans_kmedian.KmeansAssign.fit" href="#axn.ml.discrete.vl_kmeans_kmedian.KmeansAssign.fit">fit</a></code></li>
<li><code><a title="axn.ml.discrete.vl_kmeans_kmedian.KmeansAssign.get_euclidean_distance" href="#axn.ml.discrete.vl_kmeans_kmedian.KmeansAssign.get_euclidean_distance">get_euclidean_distance</a></code></li>
<li><code><a title="axn.ml.discrete.vl_kmeans_kmedian.KmeansAssign.index_lowest_inertia_cluster" href="#axn.ml.discrete.vl_kmeans_kmedian.KmeansAssign.index_lowest_inertia_cluster">index_lowest_inertia_cluster</a></code></li>
<li><code><a title="axn.ml.discrete.vl_kmeans_kmedian.KmeansAssign.inertia" href="#axn.ml.discrete.vl_kmeans_kmedian.KmeansAssign.inertia">inertia</a></code></li>
<li><code><a title="axn.ml.discrete.vl_kmeans_kmedian.KmeansAssign.predict" href="#axn.ml.discrete.vl_kmeans_kmedian.KmeansAssign.predict">predict</a></code></li>
<li><code><a title="axn.ml.discrete.vl_kmeans_kmedian.KmeansAssign.run_kmeans_initialized_centroid" href="#axn.ml.discrete.vl_kmeans_kmedian.KmeansAssign.run_kmeans_initialized_centroid">run_kmeans_initialized_centroid</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="axn.ml.discrete.vl_kmeans_kmedian.Kmedians" href="#axn.ml.discrete.vl_kmeans_kmedian.Kmedians">Kmedians</a></code></h4>
<ul class="">
<li><code><a title="axn.ml.discrete.vl_kmeans_kmedian.Kmedians.error" href="#axn.ml.discrete.vl_kmeans_kmedian.Kmedians.error">error</a></code></li>
<li><code><a title="axn.ml.discrete.vl_kmeans_kmedian.Kmedians.fit" href="#axn.ml.discrete.vl_kmeans_kmedian.Kmedians.fit">fit</a></code></li>
<li><code><a title="axn.ml.discrete.vl_kmeans_kmedian.Kmedians.predict" href="#axn.ml.discrete.vl_kmeans_kmedian.Kmedians.predict">predict</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.4</a>.</p>
</footer>
</body>
</html>