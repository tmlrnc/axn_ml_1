<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.4" />
<title>axn.ml.discrete API documentation</title>
<meta name="description" content="Discretizer Module …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>axn.ml.discrete</code></h1>
</header>
<section id="section-intro">
<p>Discretizer Module</p>
<p><img src="images/dis.png" alt="DIS"></p>
<p>To make ml models more powerful on continuous data
VL uses discretization (also known as binning).
We discretize the feature and one-hot encode the transformed data.
Note that if the bins are not reasonably wide,
there would appear to be a substantially increased risk of overfitting,
so the discretizer parameters should usually be tuned under cross validation.
After discretization, linear regression and decision tree make exactly the same prediction.
As features are constant within each bin, any model must
predict the same value for all points within a bin.
Compared with the result before discretization,
linear model become much more flexible while decision tree gets much less flexible.
Note that binning features generally has no
beneficial effect for tree-based models,
as these models can learn to split up the data anywhere.</p>
<p>Bin continuous data into intervals.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file_in</code></strong> :&ensp;<code>file</code></dt>
<dd>raw csv file input to be discretize</dd>
<dt><strong><code>file_out</code></strong> :&ensp;<code>file</code></dt>
<dd>output file of discretize process where continous data in transformed into bins used for models</dd>
<dt><strong><code>drop_column</code></strong> :&ensp;<code>string</code></dt>
<dd>drop column from discreteize process</dd>
<dt><strong><code>dicretize</code></strong> :&ensp;<code>string</code></dt>
<dd>discretization strategy - uniform, quantile analyst_supervised kmeans</dd>
<dt><strong><code>ignore</code></strong> :&ensp;<code>string</code></dt>
<dd>columns of data to NOT be encoded or discretized</dd>
<dt><strong><code>n_bins</code></strong> :&ensp;<code>int</code> or <code>array-like, shape (n_features,) (default=5)</code></dt>
<dd>The number of bins to produce. Raises ValueError if <code>n_bins &lt; 2</code>.</dd>
<dt><strong><code>encode</code></strong> :&ensp;<code>{'onehot', 'onehot-dense', 'ordinal'}, (default='onehot')</code></dt>
<dd>
<p>Method used to encode the transformed result.</p>
<p>onehot
Encode the transformed result with one-hot encoding
and return a sparse matrix. Ignored features are always
stacked to the right.
onehot-dense
Encode the transformed result with one-hot encoding
and return a dense array. Ignored features are always
stacked to the right.
ordinal
Return the bin identifier encoded as an integer value.</p>
</dd>
<dt><strong><code>strategy</code></strong> :&ensp;<code>{'uniform', 'quantile', 'kmeans'}, (default='quantile')</code></dt>
<dd>
<p>Strategy used to define the widths of the bins.</p>
<p>uniform
All bins in each feature have identical widths.
quantile
All bins in each feature have the same number of points.
kmeans
Values in each bin have the same nearest center of a 1D k-means
cluster.</p>
</dd>
<dt><strong><code>n_bins_</code></strong> :&ensp;<code>int array, shape (n_features,)</code></dt>
<dd>Number of bins per feature. Bins whose width are too small
(i.e., &lt;= 1e-8) are removed with a warning.</dd>
<dt><strong><code>bin_edges_</code></strong> :&ensp;<code>array</code> of <code>arrays, shape (n_features, )</code></dt>
<dd>The edges of each bin. Contain arrays of varying shapes <code>(n_bins_, )</code>
Ignored features will have empty arrays.</dd>
</dl>
<p>Sometimes it may be useful to convert the data back into the original
feature space. The <code>inverse_transform</code> function converts the binned
data into the original feature space. Each value will be equal to the mean
of the two bin edges.</p>
<p>DBSCAN - Density-Based Spatial Clustering of Applications with Noise.
Finds core samples of high density and expands clusters from them.
Good for data which contains clusters of similar density.</p>
<p>The maximum distance between two samples for one to be considered as in the neighborhood of the other.
This is not a maximum bound on the distances of points within a cluster. This is the most important</p>
<p>eps: Two points are considered neighbors if the distance between the two points is below the threshold epsilon.
min_samples: The minimum number of neighbors a given point should have in order to be classified as a core point.
It’s important to note that the point itself is included in the minimum number of samples.
metric: The metric to use when calculating distance between instances in a feature array (i.e. euclidean distance).</p>
<p>The algorithm works by computing the distance between every point and all other points.
We then place the points into one of three categories.</p>
<p>Core point: A point with at least min_samples points whose distance
with respect to the point is below the threshold defined by epsilon.</p>
<p>Border point: A point that isn’t in close proximity to at least min_samples points but is close enough to one or more core point.
Border points are included in the cluster of the closest core point.</p>
<p>Noise point: Points that aren’t close enough to core points to be considered border points. Noise points are ignored.
That is to say, they aren’t part of any cluster.</p>
<h2 id="returns">Returns:</h2>
<pre><code>csv file output with continous data inn bins.
</code></pre>
<h2 id="example-1-csv-files">Example 1. CSV Files:</h2>
<p>python -m discrete
</p>
<p>&ndash;file_in csvs/sales.csv </p>
<p>&ndash;file_out_ohe csvs/sales_dis.csv </p>
<p>&ndash;ignore id</p>
<p>&ndash;ignore item</p>
<h2 id="example-1-data-input-csv-file">Example 1 - Data Input CSV File:</h2>
<p><img src="images/sales.png" alt="OHE" width="600" height="300"></p>
<h2 id="example-1-one-hot-encoded-csv-file">Example 1 - One Hot Encoded CSV File:</h2>
<p><img src="images/sales_dis.png" alt="OHE" width="600" height="300"></p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Discretizer Module

&lt;img src=&#34;images/dis.png&#34; alt=&#34;DIS&#34;&gt;

To make ml models more powerful on continuous data
VL uses discretization (also known as binning).
We discretize the feature and one-hot encode the transformed data.
Note that if the bins are not reasonably wide,
there would appear to be a substantially increased risk of overfitting,
so the discretizer parameters should usually be tuned under cross validation.
After discretization, linear regression and decision tree make exactly the same prediction.
As features are constant within each bin, any model must
predict the same value for all points within a bin.
Compared with the result before discretization,
linear model become much more flexible while decision tree gets much less flexible.
Note that binning features generally has no
beneficial effect for tree-based models,
as these models can learn to split up the data anywhere.

Bin continuous data into intervals.

Parameters
----------

file_in : file
    raw csv file input to be discretize

file_out : file
    output file of discretize process where continous data in transformed into bins used for models

drop_column : string
    drop column from discreteize process



dicretize : string
    discretization strategy - uniform, quantile analyst_supervised kmeans

ignore : string
    columns of data to NOT be encoded or discretized

n_bins : int or array-like, shape (n_features,) (default=5)
    The number of bins to produce. Raises ValueError if ``n_bins &lt; 2``.

encode : {&#39;onehot&#39;, &#39;onehot-dense&#39;, &#39;ordinal&#39;}, (default=&#39;onehot&#39;)
    Method used to encode the transformed result.

    onehot
        Encode the transformed result with one-hot encoding
        and return a sparse matrix. Ignored features are always
        stacked to the right.
    onehot-dense
        Encode the transformed result with one-hot encoding
        and return a dense array. Ignored features are always
        stacked to the right.
    ordinal
        Return the bin identifier encoded as an integer value.

strategy : {&#39;uniform&#39;, &#39;quantile&#39;, &#39;kmeans&#39;}, (default=&#39;quantile&#39;)
    Strategy used to define the widths of the bins.

    uniform
        All bins in each feature have identical widths.
    quantile
        All bins in each feature have the same number of points.
    kmeans
        Values in each bin have the same nearest center of a 1D k-means
        cluster.


n_bins_ : int array, shape (n_features,)
    Number of bins per feature. Bins whose width are too small
    (i.e., &lt;= 1e-8) are removed with a warning.

bin_edges_ : array of arrays, shape (n_features, )
    The edges of each bin. Contain arrays of varying shapes ``(n_bins_, )``
    Ignored features will have empty arrays.



Sometimes it may be useful to convert the data back into the original
feature space. The ``inverse_transform`` function converts the binned
data into the original feature space. Each value will be equal to the mean
of the two bin edges.

DBSCAN - Density-Based Spatial Clustering of Applications with Noise.
Finds core samples of high density and expands clusters from them.
Good for data which contains clusters of similar density.


The maximum distance between two samples for one to be considered as in the neighborhood of the other.
This is not a maximum bound on the distances of points within a cluster. This is the most important


eps: Two points are considered neighbors if the distance between the two points is below the threshold epsilon.
min_samples: The minimum number of neighbors a given point should have in order to be classified as a core point.
It’s important to note that the point itself is included in the minimum number of samples.
metric: The metric to use when calculating distance between instances in a feature array (i.e. euclidean distance).

The algorithm works by computing the distance between every point and all other points.
We then place the points into one of three categories.

Core point: A point with at least min_samples points whose distance
with respect to the point is below the threshold defined by epsilon.

Border point: A point that isn’t in close proximity to at least min_samples points but is close enough to one or more core point.
Border points are included in the cluster of the closest core point.

Noise point: Points that aren’t close enough to core points to be considered border points. Noise points are ignored.
That is to say, they aren’t part of any cluster.


Returns:
----------
    csv file output with continous data inn bins.


Example 1. CSV Files:
---------------------
python -m discrete  \


  --file_in csvs/sales.csv \




  --file_out_ohe csvs/sales_dis.csv \


  --ignore id


  --ignore item



Example 1 - Data Input CSV File:
----------------------------
&lt;img src=&#34;images/sales.png&#34; alt=&#34;OHE&#34; width=&#34;600&#34; height=&#34;300&#34;&gt;


Example 1 - One Hot Encoded CSV File:
-----------------------------
&lt;img src=&#34;images/sales_dis.png&#34; alt=&#34;OHE&#34; width=&#34;600&#34; height=&#34;300&#34;&gt;






&#34;&#34;&#34;
# pylint: disable=invalid-name
# pylint: disable=too-many-locals
# pylint: disable=too-many-statements
# pylint: disable=import-error


import argparse
import pandas as pd
from axn.ml.discrete.discretizer import DiscretizerBuilder

description = \
    &#34;&#34;&#34;
VoterLabs Inc. Data Discretizer

READ FILE_IN_RAW.CSV
GET COLUMN HEADERS
FOR EACH COLUMN NOT IN IGNORE LIST :
GET ALL CATEGORIES = UNIQUE COLUMN VALUES
GENERATE ONE HOT ENCODING HEADER
ENCODE EACH ROW WITH 1 or 0 FOR EACH HEADER

Standardization discretization and one hot encoding is an important requirement for VL machine learning estimators.
VL models WILL behave badly if the individual features do not look like standard normally distributed data:
that is, Gaussian with zero mean and unit variance. VL ignored the distribution and just transforms the data
to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation.
This is done by transforming conntious data into bins or one hot encoding
      &#34;&#34;&#34;.strip()


def parse_command_line():
    &#34;&#34;&#34;
    reads command line and sets up program parameters
    &#34;&#34;&#34;
    parser = argparse.ArgumentParser(description=description)
    parser.add_argument(
        &#39;--file_in&#39;,
        help=&#39;raw csv file input to be discretized&#39;)
    parser.add_argument(&#39;--file_out_ohe&#39;, help=&#39;file intermediate  .&#39;)
    parser.add_argument(&#39;--file_out_discrete&#39;, help=&#39;file intermediate&#39;)
    parser.add_argument(&#39;--file_out_ohe_dis&#39;, help=&#39;file intermediate&#39;)
    parser.add_argument(
        &#39;--drop_column&#39;,
        action=&#39;append&#39;,
        help=&#39;drop column from discreteize process - BUT not from encoding or prediction&#39;)
    parser.add_argument(
        &#39;--ignore&#39;,
        action=&#39;append&#39;,
        help=&#39;columns of data to NOT be encoded or discretized &#39;)

    parser.add_argument(
        &#39;--file_out&#39;,
        help=&#39;output file of discretize process where continous data in transformed into bins used for models &#39;)
    parser.add_argument(
        &#39;--dicretize&#39;,
        nargs=&#39;+&#39;,
        action=&#39;append&#39;,
        help=&#39;discretization strategy - uniform, quantile analyst_supervised kmeans&#39;)

    args = parser.parse_args()
    return args


def main():
    &#34;&#34;&#34;


Step 1
----------
    READ FILE_IN_RAW.CSV


Step 2
----------
    GET COLUMN HEADERS



Step 3
----------
    FOR EACH COLUMN NOT IN IGNORE LIST




Step 4
----------
    GET ALL CATEGORIES = UNIQUE COLUMN VALUES



Step 5
----------
    GET COLUMN HEADERS



Step 6
----------
    DISCRTTIZE EACH ROW INTO BIN



      &#34;&#34;&#34;
    ######################################################################
    #
    # read run commands
    #
    # pylint: disable=too-many-locals
    # pylint: disable=too-many-locals
    # pylint: disable=unused-variable
    args = parse_command_line()
    file_in_name = args.file_in
    file_out_discrete = args.file_out_discrete
    file_out = args.file_out
    ignore = args.ignore

    file_out_ohe_dis = args.file_out_ohe_dis
    vl_dicretize_list_many = args.dicretize

    ######################################################################
    #
    # Discretize
    #


    print(&#34;Discretize --- START &#34;)

    drop_column = args.drop_column

    file_in_name_org = file_in_name
    file_in_name_drop = file_in_name

    dropname = &#34;_drop.csv&#34;
    file_in_name_drop = file_in_name.replace(&#34;.csv&#34;, dropname)
    dfd = pd.read_csv(file_in_name_org).fillna(value=0)
    dfd2 = dfd.drop(drop_column, axis=1)
    dfd2.to_csv(file_in_name_drop, index=False)

    file_in_name = file_in_name_drop
    file_out_org = file_out
    data_frame_all_len = pd.read_csv(file_in_name_drop).fillna(value=0)
    mycol = data_frame_all_len.columns

    my_len = len(data_frame_all_len)
    i = 0
    for vl_dicretize_list_one in vl_dicretize_list_many:
        discretizer_builder = DiscretizerBuilder(file_in_name)
        discretizer_builder.discretize(vl_dicretize_list_one)
        discretizer = discretizer_builder.build()
        discretizer.discretize()
        new_end = str(i) + &#34;.csv&#34;
        new_file = file_out_discrete.replace(&#34;.csv&#34;, new_end)
        drop = discretizer.write_discretize_csv(new_file)
        print(&#34;drop &#34; + str(drop))
        discrete_out_df = pd.read_csv(new_file)
        ohe_out_df = pd.read_csv(file_in_name)
        df_dis_ohe_result = discrete_out_df.join(ohe_out_df)
        dl = [drop]
        dll = df_dis_ohe_result.drop(dl, axis=1)
        new_end_out = str(i) + &#34;.csv&#34;
        new_file_out = file_out.replace(&#34;.csv&#34;, new_end_out)
        dll.to_csv(new_file_out, index=False)
        file_in_name = new_file_out
        i = i + 1

    dll2 = dll[:my_len]
    dll2.to_csv(file_out_org, index=False)
    print(&#34;Discretize --- END &#34;)</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="axn.ml.discrete.binize" href="binize.html">axn.ml.discrete.binize</a></code></dt>
<dd>
<div class="desc"><p>To make ml models more powerful on continuous data
VL uses discretization (also known as binning).
We discretize the feature and one-hot encode the …</p></div>
</dd>
<dt><code class="name"><a title="axn.ml.discrete.binize_kmeans" href="binize_kmeans.html">axn.ml.discrete.binize_kmeans</a></code></dt>
<dd>
<div class="desc"><p>To make ml models more powerful on continuous data
VL uses discretization (also known as binning).
We discretize the feature and one-hot encode the …</p></div>
</dd>
<dt><code class="name"><a title="axn.ml.discrete.config" href="config.html">axn.ml.discrete.config</a></code></dt>
<dd>
<div class="desc"><p>reads config yaml file into config dictionary data object
to drive the scikit learn machine learning algorithm</p></div>
</dd>
<dt><code class="name"><a title="axn.ml.discrete.discretizer" href="discretizer.html">axn.ml.discrete.discretizer</a></code></dt>
<dd>
<div class="desc"><p>Encode categorical features as a one-hot numeric array …</p></div>
</dd>
<dt><code class="name"><a title="axn.ml.discrete.generate_timeseries" href="generate_timeseries.html">axn.ml.discrete.generate_timeseries</a></code></dt>
<dd>
<div class="desc"><p>To make ml models more powerful on continuous data
VL uses discretization (also known as binning).
We discretize the feature and one-hot encode the …</p></div>
</dd>
<dt><code class="name"><a title="axn.ml.discrete.vl_kmeans_kmedian" href="vl_kmeans_kmedian.html">axn.ml.discrete.vl_kmeans_kmedian</a></code></dt>
<dd>
<div class="desc"><p>K_Means Clustering is an unsupervised machine learning method that segments similar data points into groups …</p></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="axn.ml.discrete.main"><code class="name flex">
<span>def <span class="ident">main</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="step-1">Step 1</h2>
<pre><code>READ FILE_IN_RAW.CSV
</code></pre>
<h2 id="step-2">Step 2</h2>
<pre><code>GET COLUMN HEADERS
</code></pre>
<h2 id="step-3">Step 3</h2>
<pre><code>FOR EACH COLUMN NOT IN IGNORE LIST
</code></pre>
<h2 id="step-4">Step 4</h2>
<pre><code>GET ALL CATEGORIES = UNIQUE COLUMN VALUES
</code></pre>
<h2 id="step-5">Step 5</h2>
<pre><code>GET COLUMN HEADERS
</code></pre>
<h2 id="step-6">Step 6</h2>
<pre><code>DISCRTTIZE EACH ROW INTO BIN
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def main():
    &#34;&#34;&#34;


Step 1
----------
    READ FILE_IN_RAW.CSV


Step 2
----------
    GET COLUMN HEADERS



Step 3
----------
    FOR EACH COLUMN NOT IN IGNORE LIST




Step 4
----------
    GET ALL CATEGORIES = UNIQUE COLUMN VALUES



Step 5
----------
    GET COLUMN HEADERS



Step 6
----------
    DISCRTTIZE EACH ROW INTO BIN



      &#34;&#34;&#34;
    ######################################################################
    #
    # read run commands
    #
    # pylint: disable=too-many-locals
    # pylint: disable=too-many-locals
    # pylint: disable=unused-variable
    args = parse_command_line()
    file_in_name = args.file_in
    file_out_discrete = args.file_out_discrete
    file_out = args.file_out
    ignore = args.ignore

    file_out_ohe_dis = args.file_out_ohe_dis
    vl_dicretize_list_many = args.dicretize

    ######################################################################
    #
    # Discretize
    #


    print(&#34;Discretize --- START &#34;)

    drop_column = args.drop_column

    file_in_name_org = file_in_name
    file_in_name_drop = file_in_name

    dropname = &#34;_drop.csv&#34;
    file_in_name_drop = file_in_name.replace(&#34;.csv&#34;, dropname)
    dfd = pd.read_csv(file_in_name_org).fillna(value=0)
    dfd2 = dfd.drop(drop_column, axis=1)
    dfd2.to_csv(file_in_name_drop, index=False)

    file_in_name = file_in_name_drop
    file_out_org = file_out
    data_frame_all_len = pd.read_csv(file_in_name_drop).fillna(value=0)
    mycol = data_frame_all_len.columns

    my_len = len(data_frame_all_len)
    i = 0
    for vl_dicretize_list_one in vl_dicretize_list_many:
        discretizer_builder = DiscretizerBuilder(file_in_name)
        discretizer_builder.discretize(vl_dicretize_list_one)
        discretizer = discretizer_builder.build()
        discretizer.discretize()
        new_end = str(i) + &#34;.csv&#34;
        new_file = file_out_discrete.replace(&#34;.csv&#34;, new_end)
        drop = discretizer.write_discretize_csv(new_file)
        print(&#34;drop &#34; + str(drop))
        discrete_out_df = pd.read_csv(new_file)
        ohe_out_df = pd.read_csv(file_in_name)
        df_dis_ohe_result = discrete_out_df.join(ohe_out_df)
        dl = [drop]
        dll = df_dis_ohe_result.drop(dl, axis=1)
        new_end_out = str(i) + &#34;.csv&#34;
        new_file_out = file_out.replace(&#34;.csv&#34;, new_end_out)
        dll.to_csv(new_file_out, index=False)
        file_in_name = new_file_out
        i = i + 1

    dll2 = dll[:my_len]
    dll2.to_csv(file_out_org, index=False)
    print(&#34;Discretize --- END &#34;)</code></pre>
</details>
</dd>
<dt id="axn.ml.discrete.parse_command_line"><code class="name flex">
<span>def <span class="ident">parse_command_line</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>reads command line and sets up program parameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parse_command_line():
    &#34;&#34;&#34;
    reads command line and sets up program parameters
    &#34;&#34;&#34;
    parser = argparse.ArgumentParser(description=description)
    parser.add_argument(
        &#39;--file_in&#39;,
        help=&#39;raw csv file input to be discretized&#39;)
    parser.add_argument(&#39;--file_out_ohe&#39;, help=&#39;file intermediate  .&#39;)
    parser.add_argument(&#39;--file_out_discrete&#39;, help=&#39;file intermediate&#39;)
    parser.add_argument(&#39;--file_out_ohe_dis&#39;, help=&#39;file intermediate&#39;)
    parser.add_argument(
        &#39;--drop_column&#39;,
        action=&#39;append&#39;,
        help=&#39;drop column from discreteize process - BUT not from encoding or prediction&#39;)
    parser.add_argument(
        &#39;--ignore&#39;,
        action=&#39;append&#39;,
        help=&#39;columns of data to NOT be encoded or discretized &#39;)

    parser.add_argument(
        &#39;--file_out&#39;,
        help=&#39;output file of discretize process where continous data in transformed into bins used for models &#39;)
    parser.add_argument(
        &#39;--dicretize&#39;,
        nargs=&#39;+&#39;,
        action=&#39;append&#39;,
        help=&#39;discretization strategy - uniform, quantile analyst_supervised kmeans&#39;)

    args = parser.parse_args()
    return args</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#parameters">Parameters</a></li>
<li><a href="#returns">Returns:</a></li>
<li><a href="#example-1-csv-files">Example 1. CSV Files:</a></li>
<li><a href="#example-1-data-input-csv-file">Example 1 - Data Input CSV File:</a></li>
<li><a href="#example-1-one-hot-encoded-csv-file">Example 1 - One Hot Encoded CSV File:</a></li>
</ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="axn.ml" href="../index.html">axn.ml</a></code></li>
</ul>
</li>
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="axn.ml.discrete.binize" href="binize.html">axn.ml.discrete.binize</a></code></li>
<li><code><a title="axn.ml.discrete.binize_kmeans" href="binize_kmeans.html">axn.ml.discrete.binize_kmeans</a></code></li>
<li><code><a title="axn.ml.discrete.config" href="config.html">axn.ml.discrete.config</a></code></li>
<li><code><a title="axn.ml.discrete.discretizer" href="discretizer.html">axn.ml.discrete.discretizer</a></code></li>
<li><code><a title="axn.ml.discrete.generate_timeseries" href="generate_timeseries.html">axn.ml.discrete.generate_timeseries</a></code></li>
<li><code><a title="axn.ml.discrete.vl_kmeans_kmedian" href="vl_kmeans_kmedian.html">axn.ml.discrete.vl_kmeans_kmedian</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="axn.ml.discrete.main" href="#axn.ml.discrete.main">main</a></code></li>
<li><code><a title="axn.ml.discrete.parse_command_line" href="#axn.ml.discrete.parse_command_line">parse_command_line</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.4</a>.</p>
</footer>
</body>
</html>