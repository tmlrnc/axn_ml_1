<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.4" />
<title>axn.ml.discrete.binize API documentation</title>
<meta name="description" content="To make ml models more powerful on continuous data
VL uses discretization (also known as binning).
We discretize the feature and one-hot encode the …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>axn.ml.discrete.binize</code></h1>
</header>
<section id="section-intro">
<p>To make ml models more powerful on continuous data
VL uses discretization (also known as binning).
We discretize the feature and one-hot encode the transformed data.
Note that if the bins are not reasonably wide,
there would appear to be a substantially increased risk of overfitting,
so the discretizer parameters should usually be tuned under cross validation.
After discretization, linear regression and decision tree make exactly the same prediction.
As features are constant within each bin, any model must
predict the same value for all points within a bin.
Compared with the result before discretization,
linear model become much more flexible while decision tree gets much less flexible.
Note that binning features generally has no
beneficial effect for tree-based models,
as these models can learn to split up the data anywhere.</p>
<p>Bin continuous data into intervals.
Parameters</p>
<hr>
<dl>
<dt><strong><code>n_bins</code></strong> :&ensp;<code>int</code> or <code>array-like, shape (n_features,) (default=5)</code></dt>
<dd>The number of bins to produce. Raises ValueError if <code>n_bins &lt; 2</code>.</dd>
<dt><strong><code>encode</code></strong> :&ensp;<code>{'onehot', 'onehot-dense', 'ordinal'}, (default='onehot')</code></dt>
<dd>
<p>Method used to encode the transformed result.</p>
<p>onehot
Encode the transformed result with one-hot encoding
and return a sparse matrix. Ignored features are always
stacked to the right.
onehot-dense
Encode the transformed result with one-hot encoding
and return a dense array. Ignored features are always
stacked to the right.
ordinal
Return the bin identifier encoded as an integer value.</p>
</dd>
<dt><strong><code>strategy</code></strong> :&ensp;<code>{'uniform', 'quantile', 'kmeans'}, (default='quantile')</code></dt>
<dd>
<p>Strategy used to define the widths of the bins.</p>
<p>uniform
All bins in each feature have identical widths.
quantile
All bins in each feature have the same number of points.
kmeans
Values in each bin have the same nearest center of a 1D k-means
cluster.</p>
</dd>
<dt><strong><code>n_bins_</code></strong> :&ensp;<code>int array, shape (n_features,)</code></dt>
<dd>Number of bins per feature. Bins whose width are too small
(i.e., &lt;= 1e-8) are removed with a warning.</dd>
<dt><strong><code>bin_edges_</code></strong> :&ensp;<code>array</code> of <code>arrays, shape (n_features, )</code></dt>
<dd>The edges of each bin. Contain arrays of varying shapes <code>(n_bins_, )</code>
Ignored features will have empty arrays.</dd>
</dl>
<p>Sometimes it may be useful to convert the data back into the original
feature space. The <code>inverse_transform</code> function converts the binned
data into the original feature space. Each value will be equal to the mean
of the two bin edges.</p>
<p>DBSCAN - Density-Based Spatial Clustering of Applications with Noise.
Finds core samples of high density and expands clusters from them.
Good for data which contains clusters of similar density.</p>
<p>The maximum distance between two samples for one to be considered as in the neighborhood of the other.
This is not a maximum bound on the distances of points within a cluster. This is the most important</p>
<p>eps: Two points are considered neighbors if the distance between the two points is below the threshold epsilon.
min_samples: The minimum number of neighbors a given point should have in order to be classified as a core point.
It’s important to note that the point itself is included in the minimum number of samples.
metric: The metric to use when calculating distance between instances in a feature array (i.e. euclidean distance).</p>
<p>The algorithm works by computing the distance between every point and all other points.
We then place the points into one of three categories.</p>
<p>Core point: A point with at least min_samples points whose distance
with respect to the point is below the threshold defined by epsilon.</p>
<p>Border point: A point that isn’t in close proximity to at least min_samples points but is close enough to one or more core point.
Border points are included in the cluster of the closest core point.</p>
<p>Noise point: Points that aren’t close enough to core points to be considered border points. Noise points are ignored.
That is to say, they aren’t part of any cluster.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
To make ml models more powerful on continuous data
VL uses discretization (also known as binning).
We discretize the feature and one-hot encode the transformed data.
Note that if the bins are not reasonably wide,
there would appear to be a substantially increased risk of overfitting,
so the discretizer parameters should usually be tuned under cross validation.
After discretization, linear regression and decision tree make exactly the same prediction.
As features are constant within each bin, any model must
predict the same value for all points within a bin.
Compared with the result before discretization,
linear model become much more flexible while decision tree gets much less flexible.
Note that binning features generally has no
beneficial effect for tree-based models,
as these models can learn to split up the data anywhere.

Bin continuous data into intervals.
Parameters
----------
n_bins : int or array-like, shape (n_features,) (default=5)
    The number of bins to produce. Raises ValueError if ``n_bins &lt; 2``.

encode : {&#39;onehot&#39;, &#39;onehot-dense&#39;, &#39;ordinal&#39;}, (default=&#39;onehot&#39;)
    Method used to encode the transformed result.

    onehot
        Encode the transformed result with one-hot encoding
        and return a sparse matrix. Ignored features are always
        stacked to the right.
    onehot-dense
        Encode the transformed result with one-hot encoding
        and return a dense array. Ignored features are always
        stacked to the right.
    ordinal
        Return the bin identifier encoded as an integer value.

strategy : {&#39;uniform&#39;, &#39;quantile&#39;, &#39;kmeans&#39;}, (default=&#39;quantile&#39;)
    Strategy used to define the widths of the bins.

    uniform
        All bins in each feature have identical widths.
    quantile
        All bins in each feature have the same number of points.
    kmeans
        Values in each bin have the same nearest center of a 1D k-means
        cluster.


n_bins_ : int array, shape (n_features,)
    Number of bins per feature. Bins whose width are too small
    (i.e., &lt;= 1e-8) are removed with a warning.

bin_edges_ : array of arrays, shape (n_features, )
    The edges of each bin. Contain arrays of varying shapes ``(n_bins_, )``
    Ignored features will have empty arrays.



Sometimes it may be useful to convert the data back into the original
feature space. The ``inverse_transform`` function converts the binned
data into the original feature space. Each value will be equal to the mean
of the two bin edges.

DBSCAN - Density-Based Spatial Clustering of Applications with Noise.
Finds core samples of high density and expands clusters from them.
Good for data which contains clusters of similar density.


The maximum distance between two samples for one to be considered as in the neighborhood of the other.
This is not a maximum bound on the distances of points within a cluster. This is the most important


eps: Two points are considered neighbors if the distance between the two points is below the threshold epsilon.
min_samples: The minimum number of neighbors a given point should have in order to be classified as a core point.
It’s important to note that the point itself is included in the minimum number of samples.
metric: The metric to use when calculating distance between instances in a feature array (i.e. euclidean distance).

The algorithm works by computing the distance between every point and all other points.
We then place the points into one of three categories.

Core point: A point with at least min_samples points whose distance
with respect to the point is below the threshold defined by epsilon.

Border point: A point that isn’t in close proximity to at least min_samples points but is close enough to one or more core point.
Border points are included in the cluster of the closest core point.

Noise point: Points that aren’t close enough to core points to be considered border points. Noise points are ignored.
That is to say, they aren’t part of any cluster.


&#34;&#34;&#34;
import warnings
import numbers
import numpy as np
from sklearn.utils.validation import check_array
from sklearn.utils.validation import check_is_fitted
from sklearn.utils.validation import FLOAT_DTYPES


def check_for_less(list1, val):
    &#34;&#34;&#34;
    check for least in list
    &#34;&#34;&#34;
    return all(x &lt; val for x in list1)

class VlBinizer():
    &#34;&#34;&#34;
    To make linear model more powerful on continuous data is to use discretization (also known as binning).
    We discretize the feature and one-hot encode the transformed data. Note that if the bins are not reasonably wide,
    there would appear to be a substantially increased risk of overfitting,
    so the discretizer parameters should usually be tuned under cross validation.
    After discretization, linear regression and decision tree make exactly the same prediction.
    As features are constant within each bin, any model must predict the same value for all points within a bin.
    Compared with the result before discretization, linear model become much more flexible while decision tree gets much less flexible.
    Note that binning features generally has no beneficial effect for tree-based models, as these models can learn to split up the data anywhere.

    Bin continuous data into intervals.
    Parameters
    ----------
    n_bins : int or array-like, shape (n_features,) (default=5)
        The number of bins to produce. Raises ValueError if ``n_bins &lt; 2``.

    encode : {&#39;onehot&#39;, &#39;onehot-dense&#39;, &#39;ordinal&#39;}, (default=&#39;onehot&#39;)
        Method used to encode the transformed result.

        onehot
            Encode the transformed result with one-hot encoding
            and return a sparse matrix. Ignored features are always
            stacked to the right.
        onehot-dense
            Encode the transformed result with one-hot encoding
            and return a dense array. Ignored features are always
            stacked to the right.
        ordinal
            Return the bin identifier encoded as an integer value.

    strategy : {&#39;uniform&#39;, &#39;quantile&#39;, &#39;kmeans&#39;}, (default=&#39;quantile&#39;)
        Strategy used to define the widths of the bins.

        uniform
            All bins in each feature have identical widths.
        quantile
            All bins in each feature have the same number of points.
        kmeans
            Values in each bin have the same nearest center of a 1D k-means
            cluster.


    n_bins_ : int array, shape (n_features,)
        Number of bins per feature. Bins whose width are too small
        (i.e., &lt;= 1e-8) are removed with a warning.

    bin_edges_ : array of arrays, shape (n_features, )
        The edges of each bin. Contain arrays of varying shapes ``(n_bins_, )``
        Ignored features will have empty arrays.



    Sometimes it may be useful to convert the data back into the original
    feature space. The ``inverse_transform`` function converts the binned
    data into the original feature space. Each value will be equal to the mean
    of the two bin edges.

    DBSCAN - Density-Based Spatial Clustering of Applications with Noise.
    Finds core samples of high density and expands clusters from them. Good for data which contains clusters of similar density.


The maximum distance between two samples for one to be considered as in the neighborhood of the other.
This is not a maximum bound on the distances of points within a cluster. This is the most important


    eps: Two points are considered neighbors if the distance between the two points is below the threshold epsilon.
    min_samples: The minimum number of neighbors a given point should have in order to be classified as a core point.
    It’s important to note that the point itself is included in the minimum number of samples.
    metric: The metric to use when calculating distance between instances in a feature array (i.e. euclidean distance).

The algorithm works by computing the distance between every point and all other points. We then place the points into one of three categories.

Core point: A point with at least min_samples points whose distance with respect to the point is below the threshold defined by epsilon.

Border point: A point that isn’t in close proximity to at least min_samples points but is close enough to one or more core point.
Border points are included in the cluster of the closest core point.

Noise point: Points that aren’t close enough to core points to be considered border points. Noise points are ignored.
That is to say, they aren’t part of any cluster.

    &#34;&#34;&#34;
    # pylint: disable=dangerous-default-value
    # pylint: disable=too-many-locals
    # pylint: disable=invalid-name
    # pylint: disable=singleton-comparison
    # pylint: disable=no-member
    # pylint: disable=attribute-defined-outside-init
    # pylint: disable=duplicate-code


    def __init__(
            self,
            n_bins=5,
            encode=&#39;onehot&#39;,
            strategy=&#39;quantile&#39;,
            edge_array=[]):
        self.n_bins = n_bins
        self.encode = encode
        self.strategy = strategy
        self.edge_array = edge_array

    def fit(self, x_my):
        &#34;&#34;&#34;
        Fit the estimator.

        Parameters
        ----------
        X : numeric array-like, shape (n_samples, n_features)
            Data to be discretized.

        y : None
            Ignored. This parameter exists only for compatibility with
            :class:`sklearn.pipeline.Pipeline`.

        Returns
        -------
        self
        &#34;&#34;&#34;

        x_my = check_array(x_my, dtype=&#39;numeric&#39;)

        valid_encode = (&#39;onehot&#39;, &#39;onehot-dense&#39;, &#39;ordinal&#39;)
        if self.encode not in valid_encode:
            raise ValueError(&#34;Valid options for &#39;encode&#39; are {}. &#34;
                             &#34;Got encode={!r} instead.&#34;
                             .format(valid_encode, self.encode))
        valid_strategy = (&#39;uniform&#39;, &#39;quantile&#39;, &#39;analyst_supervised&#39;, &#39;&#39;)
        if self.strategy not in valid_strategy:
            raise ValueError(&#34;Valid options for &#39;strategy&#39; are {}. &#34;
                             &#34;Got strategy={!r} instead.&#34;
                             .format(valid_strategy, self.strategy))

        n_features = x_my.shape[1]

        # FEATURES ARE COLUMS

        print(&#34;n_features &#34; + str(n_features))
        n_bins = self._validate_n_bins(n_features)
        print(&#34;n_bins &#34; + str(n_bins))
        bin_edges = np.zeros(n_features, dtype=object)
        print(&#34;bin_edges &#34; + str(bin_edges))

        for jj in range(n_features):

            column = x_my[:, jj]

            print(&#34;column &#34; + str(column))
            col_min, col_max = column.min(), column.max()

            print(&#34;col_min &#34; + str(col_min))
            print(&#34;col_max &#34; + str(col_max))

            if col_min == col_max:
                warnings.warn(&#34;Feature %d is constant and will be &#34;
                              &#34;replaced with 0.&#34; % jj)
                n_bins[jj] = 1
                bin_edges[jj] = np.array([-np.inf, np.inf])
                continue

            if self.strategy == &#39;uniform&#39;:

                print(&#34;n_bins[jj] &#34; + str(n_bins[jj]))

                # Return evenly spaced numbers over a specified interval.
                bin_edges[jj] = np.linspace(col_min, col_max, n_bins[jj] + 1)
                print(&#34;bin_edges[jj] &#34; + str(bin_edges[jj]))

            elif self.strategy == &#39;quantile&#39;:
                quantiles = np.linspace(0, 100, n_bins[jj] + 1)
                bin_edges[jj] = np.asarray(np.percentile(column, quantiles))

            elif self.strategy == &#39;analyst_supervised&#39;:

                if self.edge_array == []:
                    raise ValueError(&#34;Must have edges &#34;)

                if check_for_less(self.edge_array, col_max) == False:
                    raise ValueError(&#34;No edge bigger than number in list &#34;)

                bin_edge_manual = self.edge_array
                arr = np.array(bin_edge_manual)
                bin_edges[jj] = arr

            # Remove bins whose width are too small (i.e., &lt;= 1e-8)
            if self.strategy in &#39;quantile&#39;:
                mask = np.ediff1d(bin_edges[jj], to_begin=np.inf) &gt; 1e-8
                bin_edges[jj] = bin_edges[jj][mask]
                if len(bin_edges[jj]) - 1 != n_bins[jj]:
                    warnings.warn(&#39;Bins whose width are too small (i.e., &lt;= &#39;
                                  &#39;1e-8) in feature %d are removed. Consider &#39;
                                  &#39;decreasing the number of bins.&#39; % jj)
                    n_bins[jj] = len(bin_edges[jj]) - 1

        self.bin_edges_ = bin_edges
        self.n_bins_ = n_bins

        return self

    def _validate_n_bins(self, n_features):

        orig_bins = self.n_bins
        if isinstance(orig_bins, numbers.Number):
            if not isinstance(orig_bins, numbers.Integral):
                raise ValueError(&#34;{} received an invalid n_bins type. &#34;
                                 &#34;Received {}, expected int.&#34;
                                 .format(VlBinizer.__name__,
                                         type(orig_bins).__name__))
            if orig_bins &lt; 2:
                raise ValueError(&#34;{} received an invalid number &#34;
                                 &#34;of bins. Received {}, expected at least 2.&#34;
                                 .format(VlBinizer.__name__, orig_bins))
            return np.full(n_features, orig_bins, dtype=np.int)

        n_bins = check_array(orig_bins, dtype=np.int, copy=True,
                             ensure_2d=False)

        if n_bins.ndim &gt; 1 or n_bins.shape[0] != n_features:
            raise ValueError(&#34;n_bins must be a scalar or array &#34;
                             &#34;of shape (n_features,).&#34;)

        bad_nbins_value = (n_bins &lt; 2) | (n_bins != orig_bins)

        violating_indices = np.where(bad_nbins_value)[0]
        if violating_indices.shape[0] &gt; 0:
            indices = &#34;, &#34;.join(str(i) for i in violating_indices)
            raise ValueError(&#34;{} received an invalid number &#34;
                             &#34;of bins at indices {}. Number of bins &#34;
                             &#34;must be at least 2, and must be an int.&#34;
                             .format(VlBinizer.__name__, indices))
        return n_bins

    def transform(self, x_my):
        &#34;&#34;&#34;
        Discretize the data.

        Parameters
        ----------
        X : numeric array-like, shape (n_samples, n_features)
            Data to be discretized.

        Returns
        -------
        Xt : numeric array-like or sparse matrix
            Data in the binned space.
        &#34;&#34;&#34;
        check_is_fitted(self)
        x_myt = check_array(x_my, copy=True, dtype=FLOAT_DTYPES)
        n_features = self.n_bins_.shape[0]
        if x_myt.shape[1] != n_features:
            raise ValueError(&#34;Incorrect number of {}, &#34;
                             &#34;received {}.&#34;.format(n_features, x_myt.shape[1]))
        bin_edges = self.bin_edges_
        for jj in range(x_myt.shape[1]):

            rtol = 1.e-5
            atol = 1.e-8
            eps = atol + rtol * np.abs(x_myt[:, jj])
            x_myt[:, jj] = np.digitize(x_myt[:, jj] + eps, bin_edges[jj][1:])
        np.clip(x_myt, 0, self.n_bins_ - 1, out=x_myt)

        if self.encode == &#39;ordinal&#39;:
            return x_myt

        return self._encoder.transform(x_myt)

    def inverse_transform(self, x_myt):
        &#34;&#34;&#34;
        Transform discretized data back to original feature space.

        Note that this function does not regenerate the original data
        due to discretization rounding.

        Parameters
        ----------
        Xt : numeric array-like, shape (n_sample, n_features)
            Transformed data in the binned space.

        Returns
        -------
        Xinv : numeric array-like
            Data in the original feature space.
        &#34;&#34;&#34;
        check_is_fitted(self)

        if &#39;onehot&#39; in self.encode:
            x_myt = self._encoder.inverse_transform(x_myt)

        x_myinv = check_array(x_myt, copy=True, dtype=FLOAT_DTYPES)
        n_features = self.n_bins_.shape[0]
        if x_myinv.shape[1] != n_features:
            raise ValueError(&#34;Incorrect number of features. Expecting {}, &#34;
                             &#34;received {}.&#34;.format(n_features, x_myinv.shape[1]))

        for jj in range(n_features):
            bin_edges = self.bin_edges_[jj]
            bin_centers = (bin_edges[1:] + bin_edges[:-1]) * 0.5
            x_myinv[:, jj] = bin_centers[np.int_(x_myinv[:, jj])]

        return x_myinv</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="axn.ml.discrete.binize.check_for_less"><code class="name flex">
<span>def <span class="ident">check_for_less</span></span>(<span>list1, val)</span>
</code></dt>
<dd>
<div class="desc"><p>check for least in list</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_for_less(list1, val):
    &#34;&#34;&#34;
    check for least in list
    &#34;&#34;&#34;
    return all(x &lt; val for x in list1)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="axn.ml.discrete.binize.VlBinizer"><code class="flex name class">
<span>class <span class="ident">VlBinizer</span></span>
<span>(</span><span>n_bins=5, encode='onehot', strategy='quantile', edge_array=[])</span>
</code></dt>
<dd>
<div class="desc"><p>To make linear model more powerful on continuous data is to use discretization (also known as binning).
We discretize the feature and one-hot encode the transformed data. Note that if the bins are not reasonably wide,
there would appear to be a substantially increased risk of overfitting,
so the discretizer parameters should usually be tuned under cross validation.
After discretization, linear regression and decision tree make exactly the same prediction.
As features are constant within each bin, any model must predict the same value for all points within a bin.
Compared with the result before discretization, linear model become much more flexible while decision tree gets much less flexible.
Note that binning features generally has no beneficial effect for tree-based models, as these models can learn to split up the data anywhere.</p>
<pre><code>Bin continuous data into intervals.
Parameters
----------
n_bins : int or array-like, shape (n_features,) (default=5)
    The number of bins to produce. Raises ValueError if ``n_bins &lt; 2``.

encode : {'onehot', 'onehot-dense', 'ordinal'}, (default='onehot')
    Method used to encode the transformed result.

    onehot
        Encode the transformed result with one-hot encoding
        and return a sparse matrix. Ignored features are always
        stacked to the right.
    onehot-dense
        Encode the transformed result with one-hot encoding
        and return a dense array. Ignored features are always
        stacked to the right.
    ordinal
        Return the bin identifier encoded as an integer value.

strategy : {'uniform', 'quantile', 'kmeans'}, (default='quantile')
    Strategy used to define the widths of the bins.

    uniform
        All bins in each feature have identical widths.
    quantile
        All bins in each feature have the same number of points.
    kmeans
        Values in each bin have the same nearest center of a 1D k-means
        cluster.


n_bins_ : int array, shape (n_features,)
    Number of bins per feature. Bins whose width are too small
    (i.e., &lt;= 1e-8) are removed with a warning.

bin_edges_ : array of arrays, shape (n_features, )
    The edges of each bin. Contain arrays of varying shapes &lt;code&gt;(n\_bins\_, )&lt;/code&gt;
    Ignored features will have empty arrays.



Sometimes it may be useful to convert the data back into the original
feature space. The &lt;code&gt;inverse\_transform&lt;/code&gt; function converts the binned
data into the original feature space. Each value will be equal to the mean
of the two bin edges.

DBSCAN - Density-Based Spatial Clustering of Applications with Noise.
Finds core samples of high density and expands clusters from them. Good for data which contains clusters of similar density.
</code></pre>
<p>The maximum distance between two samples for one to be considered as in the neighborhood of the other.
This is not a maximum bound on the distances of points within a cluster. This is the most important</p>
<pre><code>eps: Two points are considered neighbors if the distance between the two points is below the threshold epsilon.
min_samples: The minimum number of neighbors a given point should have in order to be classified as a core point.
It’s important to note that the point itself is included in the minimum number of samples.
metric: The metric to use when calculating distance between instances in a feature array (i.e. euclidean distance).
</code></pre>
<p>The algorithm works by computing the distance between every point and all other points. We then place the points into one of three categories.</p>
<p>Core point: A point with at least min_samples points whose distance with respect to the point is below the threshold defined by epsilon.</p>
<p>Border point: A point that isn’t in close proximity to at least min_samples points but is close enough to one or more core point.
Border points are included in the cluster of the closest core point.</p>
<p>Noise point: Points that aren’t close enough to core points to be considered border points. Noise points are ignored.
That is to say, they aren’t part of any cluster.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class VlBinizer():
    &#34;&#34;&#34;
    To make linear model more powerful on continuous data is to use discretization (also known as binning).
    We discretize the feature and one-hot encode the transformed data. Note that if the bins are not reasonably wide,
    there would appear to be a substantially increased risk of overfitting,
    so the discretizer parameters should usually be tuned under cross validation.
    After discretization, linear regression and decision tree make exactly the same prediction.
    As features are constant within each bin, any model must predict the same value for all points within a bin.
    Compared with the result before discretization, linear model become much more flexible while decision tree gets much less flexible.
    Note that binning features generally has no beneficial effect for tree-based models, as these models can learn to split up the data anywhere.

    Bin continuous data into intervals.
    Parameters
    ----------
    n_bins : int or array-like, shape (n_features,) (default=5)
        The number of bins to produce. Raises ValueError if ``n_bins &lt; 2``.

    encode : {&#39;onehot&#39;, &#39;onehot-dense&#39;, &#39;ordinal&#39;}, (default=&#39;onehot&#39;)
        Method used to encode the transformed result.

        onehot
            Encode the transformed result with one-hot encoding
            and return a sparse matrix. Ignored features are always
            stacked to the right.
        onehot-dense
            Encode the transformed result with one-hot encoding
            and return a dense array. Ignored features are always
            stacked to the right.
        ordinal
            Return the bin identifier encoded as an integer value.

    strategy : {&#39;uniform&#39;, &#39;quantile&#39;, &#39;kmeans&#39;}, (default=&#39;quantile&#39;)
        Strategy used to define the widths of the bins.

        uniform
            All bins in each feature have identical widths.
        quantile
            All bins in each feature have the same number of points.
        kmeans
            Values in each bin have the same nearest center of a 1D k-means
            cluster.


    n_bins_ : int array, shape (n_features,)
        Number of bins per feature. Bins whose width are too small
        (i.e., &lt;= 1e-8) are removed with a warning.

    bin_edges_ : array of arrays, shape (n_features, )
        The edges of each bin. Contain arrays of varying shapes ``(n_bins_, )``
        Ignored features will have empty arrays.



    Sometimes it may be useful to convert the data back into the original
    feature space. The ``inverse_transform`` function converts the binned
    data into the original feature space. Each value will be equal to the mean
    of the two bin edges.

    DBSCAN - Density-Based Spatial Clustering of Applications with Noise.
    Finds core samples of high density and expands clusters from them. Good for data which contains clusters of similar density.


The maximum distance between two samples for one to be considered as in the neighborhood of the other.
This is not a maximum bound on the distances of points within a cluster. This is the most important


    eps: Two points are considered neighbors if the distance between the two points is below the threshold epsilon.
    min_samples: The minimum number of neighbors a given point should have in order to be classified as a core point.
    It’s important to note that the point itself is included in the minimum number of samples.
    metric: The metric to use when calculating distance between instances in a feature array (i.e. euclidean distance).

The algorithm works by computing the distance between every point and all other points. We then place the points into one of three categories.

Core point: A point with at least min_samples points whose distance with respect to the point is below the threshold defined by epsilon.

Border point: A point that isn’t in close proximity to at least min_samples points but is close enough to one or more core point.
Border points are included in the cluster of the closest core point.

Noise point: Points that aren’t close enough to core points to be considered border points. Noise points are ignored.
That is to say, they aren’t part of any cluster.

    &#34;&#34;&#34;
    # pylint: disable=dangerous-default-value
    # pylint: disable=too-many-locals
    # pylint: disable=invalid-name
    # pylint: disable=singleton-comparison
    # pylint: disable=no-member
    # pylint: disable=attribute-defined-outside-init
    # pylint: disable=duplicate-code


    def __init__(
            self,
            n_bins=5,
            encode=&#39;onehot&#39;,
            strategy=&#39;quantile&#39;,
            edge_array=[]):
        self.n_bins = n_bins
        self.encode = encode
        self.strategy = strategy
        self.edge_array = edge_array

    def fit(self, x_my):
        &#34;&#34;&#34;
        Fit the estimator.

        Parameters
        ----------
        X : numeric array-like, shape (n_samples, n_features)
            Data to be discretized.

        y : None
            Ignored. This parameter exists only for compatibility with
            :class:`sklearn.pipeline.Pipeline`.

        Returns
        -------
        self
        &#34;&#34;&#34;

        x_my = check_array(x_my, dtype=&#39;numeric&#39;)

        valid_encode = (&#39;onehot&#39;, &#39;onehot-dense&#39;, &#39;ordinal&#39;)
        if self.encode not in valid_encode:
            raise ValueError(&#34;Valid options for &#39;encode&#39; are {}. &#34;
                             &#34;Got encode={!r} instead.&#34;
                             .format(valid_encode, self.encode))
        valid_strategy = (&#39;uniform&#39;, &#39;quantile&#39;, &#39;analyst_supervised&#39;, &#39;&#39;)
        if self.strategy not in valid_strategy:
            raise ValueError(&#34;Valid options for &#39;strategy&#39; are {}. &#34;
                             &#34;Got strategy={!r} instead.&#34;
                             .format(valid_strategy, self.strategy))

        n_features = x_my.shape[1]

        # FEATURES ARE COLUMS

        print(&#34;n_features &#34; + str(n_features))
        n_bins = self._validate_n_bins(n_features)
        print(&#34;n_bins &#34; + str(n_bins))
        bin_edges = np.zeros(n_features, dtype=object)
        print(&#34;bin_edges &#34; + str(bin_edges))

        for jj in range(n_features):

            column = x_my[:, jj]

            print(&#34;column &#34; + str(column))
            col_min, col_max = column.min(), column.max()

            print(&#34;col_min &#34; + str(col_min))
            print(&#34;col_max &#34; + str(col_max))

            if col_min == col_max:
                warnings.warn(&#34;Feature %d is constant and will be &#34;
                              &#34;replaced with 0.&#34; % jj)
                n_bins[jj] = 1
                bin_edges[jj] = np.array([-np.inf, np.inf])
                continue

            if self.strategy == &#39;uniform&#39;:

                print(&#34;n_bins[jj] &#34; + str(n_bins[jj]))

                # Return evenly spaced numbers over a specified interval.
                bin_edges[jj] = np.linspace(col_min, col_max, n_bins[jj] + 1)
                print(&#34;bin_edges[jj] &#34; + str(bin_edges[jj]))

            elif self.strategy == &#39;quantile&#39;:
                quantiles = np.linspace(0, 100, n_bins[jj] + 1)
                bin_edges[jj] = np.asarray(np.percentile(column, quantiles))

            elif self.strategy == &#39;analyst_supervised&#39;:

                if self.edge_array == []:
                    raise ValueError(&#34;Must have edges &#34;)

                if check_for_less(self.edge_array, col_max) == False:
                    raise ValueError(&#34;No edge bigger than number in list &#34;)

                bin_edge_manual = self.edge_array
                arr = np.array(bin_edge_manual)
                bin_edges[jj] = arr

            # Remove bins whose width are too small (i.e., &lt;= 1e-8)
            if self.strategy in &#39;quantile&#39;:
                mask = np.ediff1d(bin_edges[jj], to_begin=np.inf) &gt; 1e-8
                bin_edges[jj] = bin_edges[jj][mask]
                if len(bin_edges[jj]) - 1 != n_bins[jj]:
                    warnings.warn(&#39;Bins whose width are too small (i.e., &lt;= &#39;
                                  &#39;1e-8) in feature %d are removed. Consider &#39;
                                  &#39;decreasing the number of bins.&#39; % jj)
                    n_bins[jj] = len(bin_edges[jj]) - 1

        self.bin_edges_ = bin_edges
        self.n_bins_ = n_bins

        return self

    def _validate_n_bins(self, n_features):

        orig_bins = self.n_bins
        if isinstance(orig_bins, numbers.Number):
            if not isinstance(orig_bins, numbers.Integral):
                raise ValueError(&#34;{} received an invalid n_bins type. &#34;
                                 &#34;Received {}, expected int.&#34;
                                 .format(VlBinizer.__name__,
                                         type(orig_bins).__name__))
            if orig_bins &lt; 2:
                raise ValueError(&#34;{} received an invalid number &#34;
                                 &#34;of bins. Received {}, expected at least 2.&#34;
                                 .format(VlBinizer.__name__, orig_bins))
            return np.full(n_features, orig_bins, dtype=np.int)

        n_bins = check_array(orig_bins, dtype=np.int, copy=True,
                             ensure_2d=False)

        if n_bins.ndim &gt; 1 or n_bins.shape[0] != n_features:
            raise ValueError(&#34;n_bins must be a scalar or array &#34;
                             &#34;of shape (n_features,).&#34;)

        bad_nbins_value = (n_bins &lt; 2) | (n_bins != orig_bins)

        violating_indices = np.where(bad_nbins_value)[0]
        if violating_indices.shape[0] &gt; 0:
            indices = &#34;, &#34;.join(str(i) for i in violating_indices)
            raise ValueError(&#34;{} received an invalid number &#34;
                             &#34;of bins at indices {}. Number of bins &#34;
                             &#34;must be at least 2, and must be an int.&#34;
                             .format(VlBinizer.__name__, indices))
        return n_bins

    def transform(self, x_my):
        &#34;&#34;&#34;
        Discretize the data.

        Parameters
        ----------
        X : numeric array-like, shape (n_samples, n_features)
            Data to be discretized.

        Returns
        -------
        Xt : numeric array-like or sparse matrix
            Data in the binned space.
        &#34;&#34;&#34;
        check_is_fitted(self)
        x_myt = check_array(x_my, copy=True, dtype=FLOAT_DTYPES)
        n_features = self.n_bins_.shape[0]
        if x_myt.shape[1] != n_features:
            raise ValueError(&#34;Incorrect number of {}, &#34;
                             &#34;received {}.&#34;.format(n_features, x_myt.shape[1]))
        bin_edges = self.bin_edges_
        for jj in range(x_myt.shape[1]):

            rtol = 1.e-5
            atol = 1.e-8
            eps = atol + rtol * np.abs(x_myt[:, jj])
            x_myt[:, jj] = np.digitize(x_myt[:, jj] + eps, bin_edges[jj][1:])
        np.clip(x_myt, 0, self.n_bins_ - 1, out=x_myt)

        if self.encode == &#39;ordinal&#39;:
            return x_myt

        return self._encoder.transform(x_myt)

    def inverse_transform(self, x_myt):
        &#34;&#34;&#34;
        Transform discretized data back to original feature space.

        Note that this function does not regenerate the original data
        due to discretization rounding.

        Parameters
        ----------
        Xt : numeric array-like, shape (n_sample, n_features)
            Transformed data in the binned space.

        Returns
        -------
        Xinv : numeric array-like
            Data in the original feature space.
        &#34;&#34;&#34;
        check_is_fitted(self)

        if &#39;onehot&#39; in self.encode:
            x_myt = self._encoder.inverse_transform(x_myt)

        x_myinv = check_array(x_myt, copy=True, dtype=FLOAT_DTYPES)
        n_features = self.n_bins_.shape[0]
        if x_myinv.shape[1] != n_features:
            raise ValueError(&#34;Incorrect number of features. Expecting {}, &#34;
                             &#34;received {}.&#34;.format(n_features, x_myinv.shape[1]))

        for jj in range(n_features):
            bin_edges = self.bin_edges_[jj]
            bin_centers = (bin_edges[1:] + bin_edges[:-1]) * 0.5
            x_myinv[:, jj] = bin_centers[np.int_(x_myinv[:, jj])]

        return x_myinv</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="axn.ml.discrete.binize.VlBinizer.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, x_my)</span>
</code></dt>
<dd>
<div class="desc"><p>Fit the estimator.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>numeric array-like, shape (n_samples, n_features)</code></dt>
<dd>Data to be discretized.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>None</code></dt>
<dd>Ignored. This parameter exists only for compatibility with
:class:<code>sklearn.pipeline.Pipeline</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>self</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, x_my):
    &#34;&#34;&#34;
    Fit the estimator.

    Parameters
    ----------
    X : numeric array-like, shape (n_samples, n_features)
        Data to be discretized.

    y : None
        Ignored. This parameter exists only for compatibility with
        :class:`sklearn.pipeline.Pipeline`.

    Returns
    -------
    self
    &#34;&#34;&#34;

    x_my = check_array(x_my, dtype=&#39;numeric&#39;)

    valid_encode = (&#39;onehot&#39;, &#39;onehot-dense&#39;, &#39;ordinal&#39;)
    if self.encode not in valid_encode:
        raise ValueError(&#34;Valid options for &#39;encode&#39; are {}. &#34;
                         &#34;Got encode={!r} instead.&#34;
                         .format(valid_encode, self.encode))
    valid_strategy = (&#39;uniform&#39;, &#39;quantile&#39;, &#39;analyst_supervised&#39;, &#39;&#39;)
    if self.strategy not in valid_strategy:
        raise ValueError(&#34;Valid options for &#39;strategy&#39; are {}. &#34;
                         &#34;Got strategy={!r} instead.&#34;
                         .format(valid_strategy, self.strategy))

    n_features = x_my.shape[1]

    # FEATURES ARE COLUMS

    print(&#34;n_features &#34; + str(n_features))
    n_bins = self._validate_n_bins(n_features)
    print(&#34;n_bins &#34; + str(n_bins))
    bin_edges = np.zeros(n_features, dtype=object)
    print(&#34;bin_edges &#34; + str(bin_edges))

    for jj in range(n_features):

        column = x_my[:, jj]

        print(&#34;column &#34; + str(column))
        col_min, col_max = column.min(), column.max()

        print(&#34;col_min &#34; + str(col_min))
        print(&#34;col_max &#34; + str(col_max))

        if col_min == col_max:
            warnings.warn(&#34;Feature %d is constant and will be &#34;
                          &#34;replaced with 0.&#34; % jj)
            n_bins[jj] = 1
            bin_edges[jj] = np.array([-np.inf, np.inf])
            continue

        if self.strategy == &#39;uniform&#39;:

            print(&#34;n_bins[jj] &#34; + str(n_bins[jj]))

            # Return evenly spaced numbers over a specified interval.
            bin_edges[jj] = np.linspace(col_min, col_max, n_bins[jj] + 1)
            print(&#34;bin_edges[jj] &#34; + str(bin_edges[jj]))

        elif self.strategy == &#39;quantile&#39;:
            quantiles = np.linspace(0, 100, n_bins[jj] + 1)
            bin_edges[jj] = np.asarray(np.percentile(column, quantiles))

        elif self.strategy == &#39;analyst_supervised&#39;:

            if self.edge_array == []:
                raise ValueError(&#34;Must have edges &#34;)

            if check_for_less(self.edge_array, col_max) == False:
                raise ValueError(&#34;No edge bigger than number in list &#34;)

            bin_edge_manual = self.edge_array
            arr = np.array(bin_edge_manual)
            bin_edges[jj] = arr

        # Remove bins whose width are too small (i.e., &lt;= 1e-8)
        if self.strategy in &#39;quantile&#39;:
            mask = np.ediff1d(bin_edges[jj], to_begin=np.inf) &gt; 1e-8
            bin_edges[jj] = bin_edges[jj][mask]
            if len(bin_edges[jj]) - 1 != n_bins[jj]:
                warnings.warn(&#39;Bins whose width are too small (i.e., &lt;= &#39;
                              &#39;1e-8) in feature %d are removed. Consider &#39;
                              &#39;decreasing the number of bins.&#39; % jj)
                n_bins[jj] = len(bin_edges[jj]) - 1

    self.bin_edges_ = bin_edges
    self.n_bins_ = n_bins

    return self</code></pre>
</details>
</dd>
<dt id="axn.ml.discrete.binize.VlBinizer.inverse_transform"><code class="name flex">
<span>def <span class="ident">inverse_transform</span></span>(<span>self, x_myt)</span>
</code></dt>
<dd>
<div class="desc"><p>Transform discretized data back to original feature space.</p>
<p>Note that this function does not regenerate the original data
due to discretization rounding.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>Xt</code></strong> :&ensp;<code>numeric array-like, shape (n_sample, n_features)</code></dt>
<dd>Transformed data in the binned space.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>Xinv</code></strong> :&ensp;<code>numeric array-like</code></dt>
<dd>Data in the original feature space.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def inverse_transform(self, x_myt):
    &#34;&#34;&#34;
    Transform discretized data back to original feature space.

    Note that this function does not regenerate the original data
    due to discretization rounding.

    Parameters
    ----------
    Xt : numeric array-like, shape (n_sample, n_features)
        Transformed data in the binned space.

    Returns
    -------
    Xinv : numeric array-like
        Data in the original feature space.
    &#34;&#34;&#34;
    check_is_fitted(self)

    if &#39;onehot&#39; in self.encode:
        x_myt = self._encoder.inverse_transform(x_myt)

    x_myinv = check_array(x_myt, copy=True, dtype=FLOAT_DTYPES)
    n_features = self.n_bins_.shape[0]
    if x_myinv.shape[1] != n_features:
        raise ValueError(&#34;Incorrect number of features. Expecting {}, &#34;
                         &#34;received {}.&#34;.format(n_features, x_myinv.shape[1]))

    for jj in range(n_features):
        bin_edges = self.bin_edges_[jj]
        bin_centers = (bin_edges[1:] + bin_edges[:-1]) * 0.5
        x_myinv[:, jj] = bin_centers[np.int_(x_myinv[:, jj])]

    return x_myinv</code></pre>
</details>
</dd>
<dt id="axn.ml.discrete.binize.VlBinizer.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, x_my)</span>
</code></dt>
<dd>
<div class="desc"><p>Discretize the data.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>numeric array-like, shape (n_samples, n_features)</code></dt>
<dd>Data to be discretized.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>Xt</code></strong> :&ensp;<code>numeric array-like</code> or <code>sparse matrix</code></dt>
<dd>Data in the binned space.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform(self, x_my):
    &#34;&#34;&#34;
    Discretize the data.

    Parameters
    ----------
    X : numeric array-like, shape (n_samples, n_features)
        Data to be discretized.

    Returns
    -------
    Xt : numeric array-like or sparse matrix
        Data in the binned space.
    &#34;&#34;&#34;
    check_is_fitted(self)
    x_myt = check_array(x_my, copy=True, dtype=FLOAT_DTYPES)
    n_features = self.n_bins_.shape[0]
    if x_myt.shape[1] != n_features:
        raise ValueError(&#34;Incorrect number of {}, &#34;
                         &#34;received {}.&#34;.format(n_features, x_myt.shape[1]))
    bin_edges = self.bin_edges_
    for jj in range(x_myt.shape[1]):

        rtol = 1.e-5
        atol = 1.e-8
        eps = atol + rtol * np.abs(x_myt[:, jj])
        x_myt[:, jj] = np.digitize(x_myt[:, jj] + eps, bin_edges[jj][1:])
    np.clip(x_myt, 0, self.n_bins_ - 1, out=x_myt)

    if self.encode == &#39;ordinal&#39;:
        return x_myt

    return self._encoder.transform(x_myt)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="axn.ml.discrete" href="index.html">axn.ml.discrete</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="axn.ml.discrete.binize.check_for_less" href="#axn.ml.discrete.binize.check_for_less">check_for_less</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="axn.ml.discrete.binize.VlBinizer" href="#axn.ml.discrete.binize.VlBinizer">VlBinizer</a></code></h4>
<ul class="">
<li><code><a title="axn.ml.discrete.binize.VlBinizer.fit" href="#axn.ml.discrete.binize.VlBinizer.fit">fit</a></code></li>
<li><code><a title="axn.ml.discrete.binize.VlBinizer.inverse_transform" href="#axn.ml.discrete.binize.VlBinizer.inverse_transform">inverse_transform</a></code></li>
<li><code><a title="axn.ml.discrete.binize.VlBinizer.transform" href="#axn.ml.discrete.binize.VlBinizer.transform">transform</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.4</a>.</p>
</footer>
</body>
</html>